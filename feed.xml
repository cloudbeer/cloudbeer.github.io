<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://youbug.cn/feed.xml" rel="self" type="application/atom+xml" /><link href="https://youbug.cn/" rel="alternate" type="text/html" /><updated>2023-12-03T05:03:20+00:00</updated><id>https://youbug.cn/feed.xml</id><title type="html">YouBug</title><subtitle>分享，记录而已</subtitle><entry><title type="html">Amazon Bedrock 内置知识库使用入门</title><link href="https://youbug.cn/2023/12/bedrock-kb.html" rel="alternate" type="text/html" title="Amazon Bedrock 内置知识库使用入门" /><published>2023-12-03T01:10:49+00:00</published><updated>2023-12-03T01:10:49+00:00</updated><id>https://youbug.cn/2023/12/bedrock-kb</id><content type="html" xml:base="https://youbug.cn/2023/12/bedrock-kb.html"><![CDATA[<p>本文是 Amazon Bedrock 自带的知识库的一个入门体验，用起来真的很方便，节省了前期的一堆劳动。</p>

<h2 id="快速入门">快速入门</h2>

<p>在年底的 re:Invent 2023 上，AWS 发布了一堆产品，其中知识库 (Knowledge base) 便提供了非常快捷的 RAG 实现路径。</p>

<p>他内置了如下的能力（截止当前文章）：</p>

<ul>
  <li>
    <p>数据源集成，当前支持 S3 存储桶里的文件</p>
  </li>
  <li>文档提取，当前支持如下格式
    <ul>
      <li>Plain text (.txt)</li>
      <li>Markdown (.md)</li>
      <li>HyperText Markup Language (.html)</li>
      <li>Microsoft Word document (.doc/.docx)</li>
      <li>Comma-separated values (.csv)</li>
      <li>Microsoft Excel spreadsheet (.xls/.xlsx)</li>
      <li>Portable Document Format (.pdf)</li>
    </ul>
  </li>
  <li>
    <p>文档分片</p>
  </li>
  <li>
    <p>向量转化（当前使用的是 Titan Embeddings G1 - Text ）</p>
  </li>
  <li>向量数据库集成，当前支持如下三种：
    <ul>
      <li>OpenSearch Serverless，支持在开通知识库的流程中创建。</li>
      <li>Pinecone</li>
      <li>Redis</li>
    </ul>
  </li>
  <li>
    <p>S3 数据源维护，当文档删除或者修改的时候可以同步向量数据库。</p>
  </li>
  <li>
    <p>在控制台提供了一个知识召回的演示</p>
  </li>
  <li>管理 API 和 运行时 API</li>
</ul>

<h2 id="实际使用集成">实际使用集成</h2>

<p>通过控制台(入口：<a href="https://console.aws.amazon.com/bedrock/home?#/knowledge-bases">https://console.aws.amazon.com/bedrock/home?#/knowledge-bases</a>)，很容易建立知识库体系。</p>

<p>当建立完成知识库之后，用户只需要将文档上传到对应的 S3 存储桶。</p>

<p>然后将知识库集成到自己的应用中，此时只需调用 运行时 API 即可。</p>

<p>运行时的 API 包含如下两个：</p>

<h3 id="retrieve">Retrieve</h3>

<p>知识召回，此方法将直接查询向量数据库，将文章片段及其 S3 的文件信息返回。</p>

<h3 id="retrieveandgenerate">RetrieveAndGenerate</h3>

<p>知识召回之后，将结果传入 生成式 AI 进行知识总结（需要结合使用 Bedrock 的生成式大语言模型，如 Claude）.</p>

<p>返回一段总结好的文字，并分段给出引用地址(citations)。</p>

<h3 id="代码示例">代码示例</h3>

<p>实际使用中，我可能需要将本地知识召回，再加上其他 Agent 得到的知识之后，再进行总结。</p>

<p>Retrieve 调用很简单，代码片段如下：</p>

<div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="p">{</span> <span class="nx">BedrockAgentRuntimeClient</span><span class="p">,</span> <span class="nx">RetrieveCommand</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">@aws-sdk/client-bedrock-agent-runtime</span><span class="dl">"</span><span class="p">;</span>
<span class="kd">const</span> <span class="nx">bedrockAgentRuntimeClient</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">BedrockAgentRuntimeClient</span><span class="p">({</span> <span class="nx">region</span> <span class="p">});</span>

<span class="k">async</span> <span class="kd">function</span> <span class="nx">retrive</span><span class="p">(</span><span class="nx">text</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="k">await</span> <span class="nx">bedrockAgentRuntimeClient</span><span class="p">.</span><span class="nx">send</span><span class="p">(</span><span class="k">new</span> <span class="nx">RetrieveCommand</span><span class="p">({</span>
    <span class="nx">knowledgeBaseId</span><span class="p">,</span>
    <span class="na">retrievalQuery</span><span class="p">:</span> <span class="p">{</span>
      <span class="nx">text</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="na">retrievalConfiguration</span><span class="p">:</span> <span class="p">{</span>
      <span class="na">vectorSearchConfiguration</span><span class="p">:</span> <span class="p">{</span>
        <span class="na">numberOfResults</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
      <span class="p">},</span>
    <span class="p">}</span>
  <span class="p">}));</span>
<span class="p">}</span>
</code></pre></div></div>

<p>稍加解释：</p>

<ul>
  <li>knowledgeBaseId 是你 Amazon Bedrock 知识库的 ID，可以从控制台寻得。</li>
  <li>text 是你提的问题。</li>
  <li>numberOfResults 是需要返回的结果条数。</li>
</ul>

<blockquote>
  <p>当前建议在实际集成中使用 JS 的 SDK。官方提供的 SDK，只有 JS 可以支持 streaming 流式输出。
使用流式输出，可以大幅减少在生成过程中的假死等待时间。</p>
</blockquote>

<p>看一下我控制台打印的结果(<code class="language-plaintext highlighter-rouge">console.log(result.retrievalResults);</code>)：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2023-12-03T04:42:40.384Z cf5e41dc-22de-402c-b4bc-86eacff68691 INFO 如何进行S3事件通知？
2023-12-03T04:42:40.385Z cf5e41dc-22de-402c-b4bc-86eacff68691 INFO <span class="o">[</span>
  <span class="o">{</span>
    content: <span class="o">{</span>
      text: <span class="s1">'For more infor...'</span>
    <span class="o">}</span>,
    location: <span class="o">{</span> s3Location: <span class="o">[</span>Object], <span class="nb">type</span>: <span class="s1">'S3'</span> <span class="o">}</span>,
    score: 0.65092564
  <span class="o">}</span>,
  <span class="o">{</span>
    content: <span class="o">{</span>
      text: <span class="s1">'Data protection in Amazon S3  PDFRSS ...'</span>
    <span class="o">}</span>,
    location: <span class="o">{</span> s3Location: <span class="o">[</span>Object], <span class="nb">type</span>: <span class="s1">'S3'</span> <span class="o">}</span>,
    score: 0.66120756
  <span class="o">}</span>,
  <span class="o">{</span>
    content: <span class="o">{</span>
      text: <span class="s1">'.  • S3 Standard, S3 Intelligent-Tiering, S3 Stand...'</span>
    <span class="o">}</span>,
    location: <span class="o">{</span> s3Location: <span class="o">[</span>Object], <span class="nb">type</span>: <span class="s1">'S3'</span> <span class="o">}</span>,
    score: 0.669181
  <span class="o">}</span>,
  <span class="o">{</span>
    content: <span class="o">{</span>
      text: <span class="s2">"You specify the Amazon Resource Na..."</span>
    <span class="o">}</span>,
    location: <span class="o">{</span> s3Location: <span class="o">[</span>Object], <span class="nb">type</span>: <span class="s1">'S3'</span> <span class="o">}</span>,
    score: 0.7088487
  <span class="o">}</span>,
  <span class="o">{</span>
    content: <span class="o">{</span>
      text: <span class="s2">"Amazon S3 Event Notifi..."</span>
    <span class="o">}</span>,
    location: <span class="o">{</span> s3Location: <span class="o">[</span>Object], <span class="nb">type</span>: <span class="s1">'S3'</span> <span class="o">}</span>,
    score: 0.7597214
  <span class="o">}</span>
<span class="o">]</span>
</code></pre></div></div>

<p>比较符合预期，接下来只需要重整这个结果，结合其他的方法获取的知识，将问题和参考知识丢给 LLM ，结合提示词工程(PE)即可完成更好的问答式输出了。</p>]]></content><author><name>啤酒云</name></author><category term="aiml" /><summary type="html"><![CDATA[本文是 Amazon Bedrock 自带的知识库的一个入门体验，用起来真的很方便，节省了前期的一堆劳动。]]></summary></entry><entry><title type="html">入门 Amazon Bedrock 只看这一篇就够了</title><link href="https://youbug.cn/2023/10/amazon-bedrock-this-is-all-you-need.html" rel="alternate" type="text/html" title="入门 Amazon Bedrock 只看这一篇就够了" /><published>2023-10-31T13:03:33+00:00</published><updated>2023-10-31T13:03:33+00:00</updated><id>https://youbug.cn/2023/10/amazon-bedrock-this-is-all-you-need</id><content type="html" xml:base="https://youbug.cn/2023/10/amazon-bedrock-this-is-all-you-need.html"><![CDATA[<p>Amazon Bedrock 简要说就是是 AWS 的一项完全托管的服务，通过 API 调用各种优质大模型。本文将总结其基本用法，并提供完整的示例。</p>

<h2 id="使用前提">使用前提</h2>

<h3 id="开通">开通</h3>

<p>到 Bedrock 后台，进入 Model access，修改一下，将你需要的模型都勾选上。然后保存，使得 Access status 为 Access granted。</p>

<h3 id="客户端类库">客户端类库</h3>

<p><strong>首先，升级相关 AWS 的类库到最新。</strong></p>

<p>本文使用 Python，需要 boto3 &gt;= 1.28.57</p>

<h3 id="设置权限">设置权限</h3>

<p>在 AWS 调用服务，总是要预先设置 IAM。</p>

<p>Bedrock 的 IAM 请参考这个链接，请灵活设置。</p>

<p><a href="https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonbedrock.html">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonbedrock.html</a></p>

<p>下面是一个示例：</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"Version"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2012-10-17"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"Statement"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
            </span><span class="nl">"Sid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"InvokeFM"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"Effect"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Allow"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"Action"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">  
               </span><span class="s2">"bedrock:GetFoundationModel"</span><span class="p">,</span><span class="w">
               </span><span class="s2">"bedrock:InvokeModel"</span><span class="p">,</span><span class="w"> 
               </span><span class="s2">"bedrock:InvokeModelWithResponseStream"</span><span class="w">
            </span><span class="p">],</span><span class="w">
            </span><span class="nl">"Resource"</span><span class="p">:</span><span class="w"> </span><span class="s2">"*"</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">]</span><span class="w">   
</span><span class="p">}</span><span class="w">         
</span></code></pre></div></div>

<h3 id="基本调用">基本调用</h3>

<p>调用方式也很简单，一眼就看明白了。</p>

<p>如果你设置好了 IAM 权限（如：本地的 AKSK 已经设置好），下面的代码可以无需修改直接运行。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="c1"># 声明客户端
</span><span class="n">bedrock</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="n">service_name</span><span class="o">=</span><span class="s">"bedrock-runtime"</span><span class="p">)</span>

<span class="c1"># 此处使用的是 Amazon 的 titan-embedding 模型
</span><span class="n">modelId</span> <span class="o">=</span> <span class="s">'amazon.titan-embed-text-v1'</span>
<span class="c1"># 使用 json 作为输入和输出的文本格式
</span><span class="n">contentType</span> <span class="o">=</span> <span class="s">'application/json'</span>
<span class="n">accept</span> <span class="o">=</span> <span class="s">'application/json'</span>

<span class="c1"># 输入参数（不同的模型要求不一样）
</span><span class="n">body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">({</span>
    <span class="s">"inputText"</span><span class="p">:</span> <span class="s">"Hello world!"</span>
<span class="p">})</span>

<span class="c1"># 发起调用
</span><span class="n">response</span> <span class="o">=</span> <span class="n">bedrock</span><span class="p">.</span><span class="n">invoke_model</span><span class="p">(</span><span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span> <span class="n">modelId</span><span class="o">=</span><span class="n">modelId</span><span class="p">,</span> <span class="n">accept</span><span class="o">=</span><span class="n">accept</span><span class="p">,</span> <span class="n">contentType</span><span class="o">=</span><span class="n">contentType</span><span class="p">,)</span>

<span class="c1"># 把结果弄出来
</span><span class="n">response_body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'body'</span><span class="p">).</span><span class="n">read</span><span class="p">())</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">response_body</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'embedding'</span><span class="p">)</span>

<span class="c1"># 搞定
</span><span class="k">print</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
</code></pre></div></div>

<p>如果模型支持 stream 输出，则是如下的代码，修改调用方法为 <code class="language-plaintext highlighter-rouge">invoke_model_with_response_stream</code>，并解析输出 stream：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="c1"># 声明客户端
</span><span class="n">bedrock</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="n">service_name</span><span class="o">=</span><span class="s">"bedrock-runtime"</span><span class="p">)</span>

<span class="c1"># 使用的是 Claude v2 模型
</span><span class="n">modelId</span> <span class="o">=</span> <span class="s">'anthropic.claude-v2'</span>
<span class="c1"># 使用 json 作为输入的文本格式
</span><span class="n">contentType</span> <span class="o">=</span> <span class="s">'application/json'</span>
<span class="c1"># stream 输出，有可能一次输出的 json 字符串会被截断
</span><span class="n">accept</span> <span class="o">=</span> <span class="s">'*/*'</span>

<span class="c1"># 输入参数（不同的模型要求不一样）
</span><span class="n">body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">({</span>
  <span class="s">"prompt"</span><span class="p">:</span> <span class="s">"Human:你是谁?请介绍自己，不少于100字</span><span class="se">\n\n</span><span class="s">Assistant:"</span><span class="p">,</span>
  <span class="s">"max_tokens_to_sample"</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
  <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
  <span class="s">"top_p"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
<span class="p">})</span>

<span class="c1"># 发起调用
</span><span class="n">response</span> <span class="o">=</span> <span class="n">bedrock</span><span class="p">.</span><span class="n">invoke_model_with_response_stream</span><span class="p">(</span>
    <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span>
    <span class="n">modelId</span><span class="o">=</span><span class="n">modelId</span><span class="p">,</span>
    <span class="n">accept</span><span class="o">=</span><span class="n">accept</span><span class="p">,</span>
    <span class="n">contentType</span><span class="o">=</span><span class="n">contentType</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"body"</span><span class="p">)</span>

<span class="c1"># 输出 stream
</span><span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
        <span class="n">chunk</span> <span class="o">=</span> <span class="n">event</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"chunk"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">chunk</span><span class="p">:</span>
            <span class="n">chunk_obj</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">chunk</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"bytes"</span><span class="p">).</span><span class="n">decode</span><span class="p">())</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">chunk_obj</span><span class="p">[</span><span class="s">"completion"</span><span class="p">]</span>
            <span class="k">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div></div>

<p>然后，通过修改 modelId 和 body 的参数就可以完成各种目标了。你需要去查看 Bedrock 调用的各个模型文档，找到对应的参数就好了。</p>

<p>到这里，其实已经讲完 bedrock 的调用了。怎么样？很简单吧！</p>

<p>希望你现在已经去找文档并动手去写代码了！</p>

<p>下面是当前 Bedrock 支持的模型，等你需要参考的时候，可以回头来看下面的内容。</p>

<blockquote>
  <p>Bedrock 会飞快迭代，说不定等你再回来时，下面的内容就过时了。</p>
</blockquote>

<h2 id="bedrock-支持的模型及参数">Bedrock 支持的模型及参数</h2>

<p>首先你可以 list 一下，看看支持的模型和 modelId.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws bedrock list-foundation-models <span class="nt">--region</span><span class="o">=</span>&lt;region&gt;
</code></pre></div></div>

<p>具体的模型参数可以在控制台看到，AWS 的 Web 控制台贴心的给每个模型写了基本的输入参数。</p>

<p>从 Bedrock 后台，进入 Base Models 点开每个 model 的详情，就能看到调用参数了，如：<a href="https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/models">https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/models</a></p>

<p>以下的总结大部分为搬运，有些参数官方文档也没发出来，仅供参考。一切以官方文档为准。</p>

<blockquote>
  <p>如果官方文档的来不及更新，并且你在 2023 年 11 月初看到这篇文章，应该以我的代码为准，因为我都测试了。</p>
</blockquote>

<h3 id="claude">Claude</h3>

<ul>
  <li>Max tokens: 100k</li>
  <li>多语言</li>
</ul>

<p>调用代码请参考上文中的 invoke_model_with_response_stream</p>

<h4 id="claude-v2">Claude v2</h4>

<p>modelId 为 <code class="language-plaintext highlighter-rouge">anthropic.claude-v2</code></p>

<h4 id="claude-instance-v12">Claude Instance v1.2</h4>

<h4 id="modelid-为-anthropicclaude-instant-v1">modelId 为 <code class="language-plaintext highlighter-rouge">anthropic.claude-instant-v1</code></h4>

<h4 id="claude-v13">Claude v1.3</h4>

<p>modelId 为 <code class="language-plaintext highlighter-rouge">anthropic.claude-v1</code></p>

<h3 id="cohere">Cohere</h3>

<h4 id="command">Command</h4>

<ul>
  <li>Model size: 52B</li>
  <li>Max tokens: 4096</li>
  <li>英语</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">modelId</span> <span class="o">=</span> <span class="s">'cohere.command-text-v14'</span>
<span class="n">accept</span> <span class="o">=</span> <span class="s">'*/*'</span>
<span class="n">contentType</span> <span class="o">=</span> <span class="s">'application/json'</span>

<span class="k">def</span> <span class="nf">ask</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">):</span>
  <span class="n">body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">({</span>
    <span class="s">"prompt"</span><span class="p">:</span> <span class="n">q</span><span class="p">,</span>
    <span class="s">"max_tokens"</span><span class="p">:</span> <span class="n">max_tokens</span><span class="p">,</span>
    <span class="s">"temperature"</span><span class="p">:</span> <span class="n">temperature</span>
  <span class="p">})</span>
  <span class="n">response</span> <span class="o">=</span> <span class="n">bedrock</span><span class="p">.</span><span class="n">invoke_model</span><span class="p">(</span><span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span> <span class="n">modelId</span><span class="o">=</span><span class="n">modelId</span><span class="p">,</span> <span class="n">accept</span><span class="o">=</span><span class="n">accept</span><span class="p">,</span> <span class="n">contentType</span><span class="o">=</span><span class="n">contentType</span><span class="p">,)</span>
  <span class="c1"># print(response)
</span>  
  <span class="n">response_body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'body'</span><span class="p">).</span><span class="n">read</span><span class="p">())</span>
  <span class="c1"># print(response_body)
</span>  
  <span class="k">return</span> <span class="n">response_body</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'generations'</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s">'text'</span><span class="p">]</span>

<span class="c1"># 测试调用
</span><span class="k">print</span><span class="p">(</span><span class="n">ask</span><span class="p">(</span><span class="s">"""Translate this sentence to Chinese: Are you going to Scarborough Fair: Parsley, sage, rosemary and thyme."""</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># 试试中文，貌似能听懂，但不太会说
</span><span class="k">print</span><span class="p">(</span><span class="n">ask</span><span class="p">(</span><span class="s">"请翻译为英文：君不见黄河之水天上来"</span><span class="p">))</span>

</code></pre></div></div>

<h3 id="jurassic">Jurassic</h3>

<ul>
  <li>Max tokens: 8191</li>
  <li>English, Spanish, French, German, Portuguese, Italian, Dutch</li>
</ul>

<p>Jurassic 的入参和出参规范不太一样（入参是驼峰，claude2 是 snake），模型参数也和说明和文档稍有出入，看起来还在迭代。</p>

<p>经过一些修改，下面是完整可运行的代码。</p>

<h4 id="jurassic-2-ultra">Jurassic-2 Ultra</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">modelId</span> <span class="o">=</span> <span class="s">'ai21.j2-ultra-v1'</span>
<span class="n">accept</span> <span class="o">=</span> <span class="s">'*/*'</span>
<span class="n">contentType</span> <span class="o">=</span> <span class="s">'application/json'</span>

<span class="k">def</span> <span class="nf">ask</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">modelId</span><span class="o">=</span><span class="n">modelId</span><span class="p">,</span> <span class="n">maxTokens</span><span class="o">=</span><span class="mi">300</span><span class="p">):</span>
  <span class="n">body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">({</span>
    <span class="s">"prompt"</span><span class="p">:</span> <span class="n">q</span><span class="p">,</span>
    <span class="s">"maxTokens"</span><span class="p">:</span> <span class="n">maxTokens</span><span class="p">,</span>
    <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
    <span class="s">"topP"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s">"stopSequences"</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s">"countPenalty"</span><span class="p">:{</span>
      <span class="s">"scale"</span><span class="p">:</span><span class="mi">0</span>
    <span class="p">},</span>
    <span class="s">"presencePenalty"</span><span class="p">:{</span>
      <span class="s">"scale"</span><span class="p">:</span><span class="mi">0</span>
    <span class="p">},</span>
    <span class="s">"frequencyPenalty"</span><span class="p">:{</span>
      <span class="s">"scale"</span><span class="p">:</span><span class="mi">0</span>
    <span class="p">}</span>
  <span class="p">})</span>


  <span class="n">response</span> <span class="o">=</span> <span class="n">bedrock</span><span class="p">.</span><span class="n">invoke_model</span><span class="p">(</span><span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span> <span class="n">modelId</span><span class="o">=</span><span class="n">modelId</span><span class="p">,</span> <span class="n">accept</span><span class="o">=</span><span class="n">accept</span><span class="p">,</span> <span class="n">contentType</span><span class="o">=</span><span class="n">contentType</span><span class="p">,)</span>
  <span class="n">response_body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'body'</span><span class="p">).</span><span class="n">read</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">response_body</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'completions'</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">"data"</span><span class="p">).</span><span class="n">get</span><span class="p">(</span><span class="s">'text'</span><span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="n">ask</span><span class="p">(</span><span class="s">"Who are you?"</span><span class="p">))</span>

<span class="c1"># 试试中文
</span><span class="k">print</span><span class="p">(</span><span class="n">ask</span><span class="p">(</span><span class="s">"你是谁?"</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="jurassic-2-mid">Jurassic-2 Mid</h4>

<p>modelId 为 <code class="language-plaintext highlighter-rouge">ai21.j2-mid-v1</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">ask</span><span class="p">(</span><span class="s">"Who are you?"</span><span class="p">,</span> <span class="n">modelId</span><span class="o">=</span><span class="s">"ai21.j2-mid-v1"</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">ask</span><span class="p">(</span><span class="s">"请翻译为英文：白日依山尽"</span><span class="p">,</span> <span class="n">modelId</span><span class="o">=</span><span class="s">"ai21.j2-mid-v1"</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="titan">Titan</h3>

<p>Titan 有三个模型，目前只有 Embeddings 生成可以测试。</p>

<h4 id="titan-embeddings-generation-1-g1">Titan Embeddings Generation 1 (G1)</h4>

<ul>
  <li>Max tokens: 8k</li>
  <li>输出向量维度: 1536</li>
  <li>多语言支持</li>
</ul>

<p>下面代码的尝试使用 Titan 进行向量化，并进行相似度比较。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">modelId</span> <span class="o">=</span> <span class="s">'amazon.titan-embed-text-v1'</span>
<span class="n">accept</span> <span class="o">=</span> <span class="s">'application/json'</span>
<span class="n">contentType</span> <span class="o">=</span> <span class="s">'application/json'</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="n">body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">({</span>
      <span class="s">"inputText"</span><span class="p">:</span> <span class="n">text</span>
  <span class="p">})</span>
  <span class="n">response</span> <span class="o">=</span> <span class="n">bedrock</span><span class="p">.</span><span class="n">invoke_model</span><span class="p">(</span><span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span> <span class="n">modelId</span><span class="o">=</span><span class="n">modelId</span><span class="p">,</span> <span class="n">accept</span><span class="o">=</span><span class="n">accept</span><span class="p">,</span> <span class="n">contentType</span><span class="o">=</span><span class="n">contentType</span><span class="p">,)</span>
  
  <span class="n">response_body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'body'</span><span class="p">).</span><span class="n">read</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">response_body</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'embedding'</span><span class="p">)</span>
</code></pre></div></div>

<p>定义相似度的函数</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dot</span><span class="p">,</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">cosine_sim</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span><span class="o">*</span><span class="n">norm</span><span class="p">(</span><span class="n">v2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">l2_sim</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">norm</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">v1</span><span class="p">))</span><span class="o">-</span><span class="n">array</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">v2</span><span class="p">)))</span>

</code></pre></div></div>

<p>测试</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word</span> <span class="o">=</span> <span class="s">"对不起"</span>
<span class="n">compares</span> <span class="o">=</span> <span class="p">[</span><span class="s">"对不起"</span><span class="p">,</span> <span class="s">"抱歉"</span><span class="p">,</span> <span class="s">"對不起"</span><span class="p">,</span> <span class="s">"對唔住"</span><span class="p">,</span><span class="s">"真遗憾"</span><span class="p">,</span> <span class="s">"吃了吗"</span><span class="p">,</span> <span class="s">"I'm sorry"</span><span class="p">,</span> <span class="s">"I apologize"</span><span class="p">,</span> <span class="s">"the weather"</span><span class="p">,</span>  <span class="s">"ごめんなさい"</span><span class="p">,</span> <span class="s">"미안해"</span><span class="p">,</span> <span class="s">"Désolé"</span><span class="p">]</span>

<span class="n">wordvec</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="k">for</span> <span class="n">compare</span> <span class="ow">in</span> <span class="n">compares</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s"> 与 </span><span class="si">{</span><span class="n">compare</span><span class="si">}</span><span class="s"> 的 Cosine 相似度："</span><span class="p">,</span>  <span class="n">cosine_sim</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">compare</span><span class="p">),</span> <span class="n">wordvec</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>

<span class="k">for</span> <span class="n">compare</span> <span class="ow">in</span> <span class="n">compares</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s"> 与 </span><span class="si">{</span><span class="n">compare</span><span class="si">}</span><span class="s"> 的 L2 距离："</span><span class="p">,</span>  <span class="n">l2_sim</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">compare</span><span class="p">),</span> <span class="n">wordvec</span><span class="p">))</span>
</code></pre></div></div>

<p>从向量结果比较看来，<ruby>
大<rt>不</rt>
概<rt>太</rt>
<ruby>符合预期。</ruby></ruby></p>

<h2 id="stability-ai">Stability AI</h2>

<h3 id="stable-diffusion-xl">Stable Diffusion XL</h3>

<p>当前支持的版本为 v0.8。</p>

<p>输入参数可以参考 <a href="https://dreamstudio.ai/">DreamStudio</a></p>

<p>代码参考：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># 修改此处的 region_name 可以更换 region
</span><span class="n">bedrock</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="n">service_name</span><span class="o">=</span><span class="s">'bedrock-runtime'</span><span class="p">,</span><span class="n">region_name</span><span class="o">=</span><span class="s">'us-west-2'</span><span class="p">)</span>

<span class="c1"># 可选 style，此处为 DreamStudio 的列表参考，并未调用
</span><span class="n">style_presets</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">"enhance"</span><span class="p">,</span>
    <span class="s">"anime"</span><span class="p">,</span>
    <span class="s">"photographic"</span><span class="p">,</span>
    <span class="s">"digital-art"</span><span class="p">,</span>
    <span class="s">"comic-book"</span><span class="p">,</span>
    <span class="s">"fantasy-art"</span><span class="p">,</span>
    <span class="s">"analog-film"</span><span class="p">,</span>
    <span class="s">"neon-punk"</span><span class="p">,</span>
    <span class="s">"isometric"</span><span class="p">,</span>
    <span class="s">"low-poly"</span><span class="p">,</span>
    <span class="s">"origami"</span><span class="p">,</span>
    <span class="s">"line-art"</span><span class="p">,</span>
    <span class="s">"craft-clay"</span><span class="p">,</span>
    <span class="s">"cinematic"</span><span class="p">,</span>
    <span class="s">"3d-model"</span><span class="p">,</span>
    <span class="s">"pixel-art"</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">negative_prompts</span><span class="o">=</span><span class="p">[</span><span class="s">"poorly rendered"</span><span class="p">],</span> <span class="n">style_preset</span><span class="o">=</span><span class="s">"enhance"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">)):</span>
  <span class="n">request</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">({</span>
    <span class="s">"text_prompts"</span><span class="p">:</span> <span class="p">(</span>
        <span class="p">[{</span><span class="s">"text"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span> <span class="s">"weight"</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}]</span>
        <span class="o">+</span> <span class="p">[{</span><span class="s">"text"</span><span class="p">:</span> <span class="n">negprompt</span><span class="p">,</span> <span class="s">"weight"</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">}</span> <span class="k">for</span> <span class="n">negprompt</span> <span class="ow">in</span> <span class="n">negative_prompts</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="s">"cfg_scale"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="c1"># "seed": -1,
</span>    <span class="s">"steps"</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s">"style_preset"</span><span class="p">:</span> <span class="n">style_preset</span><span class="p">,</span>
    <span class="s">"width"</span><span class="p">:</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s">"height"</span><span class="p">:</span> <span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
  <span class="p">})</span>
  <span class="n">modelId</span> <span class="o">=</span> <span class="s">"stability.stable-diffusion-xl"</span>
  <span class="n">response</span> <span class="o">=</span> <span class="n">bedrock</span><span class="p">.</span><span class="n">invoke_model</span><span class="p">(</span><span class="n">body</span><span class="o">=</span><span class="n">request</span><span class="p">,</span> <span class="n">modelId</span><span class="o">=</span><span class="n">modelId</span><span class="p">)</span>
  <span class="n">response_body</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"body"</span><span class="p">).</span><span class="n">read</span><span class="p">())</span>

  <span class="n">base_64_img_str</span> <span class="o">=</span> <span class="n">response_body</span><span class="p">[</span><span class="s">"artifacts"</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">"base64"</span><span class="p">)</span>
  <span class="n">image_1</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">io</span><span class="p">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">base64</span><span class="p">.</span><span class="n">decodebytes</span><span class="p">(</span><span class="nb">bytes</span><span class="p">(</span><span class="n">base_64_img_str</span><span class="p">,</span> <span class="s">"utf-8"</span><span class="p">))))</span>
  <span class="k">return</span> <span class="n">image_1</span>

</code></pre></div></div>

<p>调用：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predict</span><span class="p">(</span>
    <span class="s">"3D product render, futuristic sofa, finely detailed, purism, ue 5, a computer rendering, minimalism, octane render, 4k"</span><span class="p">,</span> 
    <span class="n">negative_prompts</span><span class="o">=</span><span class="p">[</span>
        <span class="s">"poorly rendered"</span><span class="p">,</span>
        <span class="s">"worst quality"</span><span class="p">,</span>
        <span class="s">"monochrome"</span><span class="p">,</span>
        <span class="s">"cropped"</span><span class="p">,</span>
        <span class="s">"duplicate"</span><span class="p">,</span>
        <span class="s">"out of frame"</span><span class="p">,</span> 
        <span class="s">"ugly"</span><span class="p">,</span> 
        <span class="s">"deformed"</span><span class="p">,</span>
        <span class="s">"complex background"</span>
    <span class="p">],</span>
    <span class="n">style_preset</span><span class="o">=</span><span class="s">"3d-model"</span><span class="p">,</span> 
    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>当前的 SDXL 模型有下面的优点和坑：</p>

<ul>
  <li>支持 style_preset 参数，这个参数在官方文档里没写，官方 sample notebook 有。</li>
  <li>可以完美输出 512 像素的图片，1024 的反倒无法输出。</li>
  <li>必须有一个边是 512 或者 768，另一个边长是 8 的倍数(这个 SD 都一样)。</li>
</ul>

<h2 id="总结和继续">总结和继续</h2>

<p>亚麻的 Bedrock 集合了生成式 AI 各家（老二，老三们）之所长，并坚持不断更新迭代，终将进化为地球的最强者。</p>

<p>你需要做的就是了解各个模型的特点，找到合适的提示词模板（提示词工程 PE），加上预处理（用各种类库），然后组合成自动化流程。</p>

<p>最终，漂亮的应用呼之欲出。</p>

<p>祝你成功！！！</p>

<hr />

<p>模型参数： <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html">https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html</a></p>

<p><a href="https://github.com/aws-samples/amazon-bedrock-workshop">Amazon Bedrock Workshop</a></p>]]></content><author><name>啤酒云</name></author><category term="aiml," /><category term="aws" /><summary type="html"><![CDATA[Amazon Bedrock 简要说就是是 AWS 的一项完全托管的服务，通过 API 调用各种优质大模型。本文将总结其基本用法，并提供完整的示例。]]></summary></entry><entry><title type="html">使用 KeyBERT 进行关键字提取</title><link href="https://youbug.cn/2023/09/keywords-by-keybert.html" rel="alternate" type="text/html" title="使用 KeyBERT 进行关键字提取" /><published>2023-09-14T09:10:49+00:00</published><updated>2023-09-14T09:10:49+00:00</updated><id>https://youbug.cn/2023/09/keywords-by-keybert</id><content type="html" xml:base="https://youbug.cn/2023/09/keywords-by-keybert.html"><![CDATA[<p>在知识库搜索/问答场景中，用户输入的搜索内容有可能是一个完整的句子，在这个情况下，进行向量化之前，一般建议要进行关键字提取或者意图识别。在专业领域，一般可以训练一个模型来进行此项工作。使用模型提取关键字，可以使用 KeyBERT 这个库来完成。</p>

<h2 id="开始">开始</h2>

<p>首先安装依赖库: keybert</p>

<h2 id="英文提取">英文提取</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>

<span class="n">doc</span> <span class="o">=</span> <span class="s">"""
      Can you tell me how much the newest tesla model3?
      """</span>
<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">kw_model</span><span class="p">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>

</code></pre></div></div>

<p>结果为：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('tesla', 0.5866), ('model3', 0.519), ('newest', 0.315), ('tell', 0.1536)]
</code></pre></div></div>

<p>貌似还不错。</p>

<h2 id="试试中文">试试中文</h2>

<p>改下 doc 的值。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>

<span class="n">doc</span> <span class="o">=</span> <span class="s">"""
      你能告诉我最新款的特斯拉model3的价格吗?
      """</span>
<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">kw_model</span><span class="p">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>

</code></pre></div></div>

<p>结果为：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('你能告诉我最新款的特斯拉model3的价格吗', 0.953)]
</code></pre></div></div>

<p>这…</p>

<h2 id="原来可以中文">原来可以中文</h2>

<p><a href="https://maartengr.github.io/KeyBERT/guides/countvectorizer.html#tokenizer">https://maartengr.github.io/KeyBERT/guides/countvectorizer.html#tokenizer</a> 可以支持 jieba 进行分词。</p>

<p>安装依赖：jieba，sklearn</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="k">def</span> <span class="nf">tokenize_zh</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="p">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">words</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenize_zh</span><span class="p">)</span>


<span class="n">doc</span> <span class="o">=</span> <span class="s">"""
      你能告诉我最新款的特斯拉model3的价格吗?
      """</span>

<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">kw_model</span><span class="p">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">=</span><span class="n">vectorizer</span><span class="p">)</span>
</code></pre></div></div>

<p>结果为：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('model3', 0.5499),
 ('最新款', 0.5385),
 ('的', 0.4434),
 ('告诉', 0.3991),
 ('价格', 0.3991)]
</code></pre></div></div>

<p>可以啊！</p>

<h2 id="sentence-transformer">Sentence Transformer</h2>

<p>可以直接加载  Sentence Transformer 模型</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="k">def</span> <span class="nf">tokenize_zh</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="p">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">words</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenize_zh</span><span class="p">)</span>


<span class="n">doc</span> <span class="o">=</span> <span class="s">"""
      你能告诉我最新款的特斯拉model3的价格吗?
      """</span>

<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s">"sentence-transformers/paraphrase-multilingual-mpnet-base-v2"</span><span class="p">)</span>
<span class="n">kw_model</span><span class="p">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">=</span><span class="n">vectorizer</span><span class="p">)</span>
</code></pre></div></div>

<p>结果为：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('特斯拉', 0.6761),
 ('model3', 0.5485),
 ('最新款', 0.4465),
 ('价格', 0.4168),
 ('告诉', 0.292)]
</code></pre></div></div>

<p>从排序看，感觉 paraphrase-multilingual-mpnet-base-v2 更合我意啊。</p>

<p>Sentence Transformer 也可以这么用：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s">'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda'</span><span class="p">)</span>
<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p>输出结果和上面的一样。</p>

<h2 id="更多模型的支持">更多模型的支持</h2>

<p>参考这里，<a href="https://maartengr.github.io/KeyBERT/guides/embeddings.html#hugging-face-transformers">https://maartengr.github.io/KeyBERT/guides/embeddings.html#hugging-face-transformers</a></p>

<p>真是个好东西。</p>

<h2 id="next">Next</h2>

<p>然后，只需要把提取出来的关键词丢给向量模型，剔除了干扰词的向量，搜索出来的结果质量肯定会大大提高。</p>

<hr />
<p>参考</p>

<p><a href="https://maartengr.github.io/KeyBERT/">https://maartengr.github.io/KeyBERT/</a></p>]]></content><author><name>啤酒云</name></author><category term="aws," /><category term="aiml" /><summary type="html"><![CDATA[在知识库搜索/问答场景中，用户输入的搜索内容有可能是一个完整的句子，在这个情况下，进行向量化之前，一般建议要进行关键字提取或者意图识别。在专业领域，一般可以训练一个模型来进行此项工作。使用模型提取关键字，可以使用 KeyBERT 这个库来完成。]]></summary></entry><entry><title type="html">在 AWS Inferentia 2 上使用 Stable Diffusion</title><link href="https://youbug.cn/2023/08/stable-diffusion-on-aws-inf2.html" rel="alternate" type="text/html" title="在 AWS Inferentia 2 上使用 Stable Diffusion" /><published>2023-08-31T05:10:49+00:00</published><updated>2023-08-31T05:10:49+00:00</updated><id>https://youbug.cn/2023/08/stable-diffusion-on-aws-inf2</id><content type="html" xml:base="https://youbug.cn/2023/08/stable-diffusion-on-aws-inf2.html"><![CDATA[<p>AWS Inferentia2 实例专为深度学习（DL）推理而构建。它们在 Amazon EC2 中以最低的成本为生成式人工智能（AI）模型（包括大型语言模型（LLM）和视觉转换器）提供高性能计算。您可以使用 Inf2 实例来运行推理应用程序，以实现文本摘要、代码生成、视频和图像生成、语音识别、个性化、欺诈检测等等。</p>

<h2 id="启动实例">启动实例</h2>

<p>启动 Inf2 实例需要选择专门的系统 AMI，在 AMI 市场搜索 Neuron，选择 AMI： <code class="language-plaintext highlighter-rouge">Deep Learning AMI Neuron PyTorch 1.13 (Ubuntu 20.04)</code> - 截止 2023-08-31</p>

<p>由于需要转化模型，官方的 sample 实例建议机型为 <code class="language-plaintext highlighter-rouge">inf2.8xlarge</code>。</p>

<p>进入系统后，查看 <code class="language-plaintext highlighter-rouge">/home/ubuntu/README</code> 文件，首先启动 PyTorch 的 Inf2 环境：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> /opt/aws_neuron_venv_pytorch/bin/activate
</code></pre></div></div>

<h2 id="stable-diffusion-15">Stable diffusion 1.5</h2>

<h3 id="模型转换">模型转换</h3>

<p>使用如下的代码将模型转化为 Inf2 的支持的模型。</p>

<p>此代码来自于官方示例。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"NEURON_FUSE_SOFTMAX"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"1"</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch_neuronx</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">image</span> <span class="k">as</span> <span class="n">mpimg</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>

<span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">StableDiffusionPipeline</span>
<span class="kn">from</span> <span class="nn">diffusers.models.unet_2d_condition</span> <span class="kn">import</span> <span class="n">UNet2DConditionOutput</span>

<span class="kn">from</span> <span class="nn">diffusers.models.cross_attention</span> <span class="kn">import</span> <span class="n">CrossAttention</span>

<span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1">###### 定义参数 #########
</span><span class="n">DTYPE</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>
<span class="n">COMPILER_WORKDIR_ROOT</span> <span class="o">=</span> <span class="s">'/home/ubuntu/models/deli-inf2'</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s">"XpucT/Deliberate"</span>
<span class="c1">#########################
</span>
<span class="c1">######## 类和方法的定义 #################
</span><span class="k">def</span> <span class="nf">get_attention_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">dtype</span>

    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">upcast_attention</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>

    <span class="k">if</span><span class="p">(</span><span class="n">query</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">key</span><span class="p">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">cust_badbmm</span><span class="p">(</span>
            <span class="n">key</span><span class="p">,</span>
            <span class="n">query</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">upcast_softmax</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">cust_badbmm</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">upcast_softmax</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">attention_probs</span>

<span class="k">def</span> <span class="nf">cust_badbmm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">bmm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">scaled</span> <span class="o">=</span> <span class="n">bmm</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="k">return</span> <span class="n">scaled</span>

<span class="k">class</span> <span class="nc">UNetWrap</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unet</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">unet</span> <span class="o">=</span> <span class="n">unet</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">out_tuple</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">unet</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_tuple</span>

<span class="k">class</span> <span class="nc">NeuronUNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unetwrap</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">unetwrap</span> <span class="o">=</span> <span class="n">unetwrap</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">unetwrap</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">config</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">unetwrap</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">in_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">unetwrap</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">device</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">unetwrap</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">timestep</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">expand</span><span class="p">((</span><span class="n">sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)),</span> <span class="n">encoder_hidden_states</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">UNet2DConditionOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">sample</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">NeuronTextEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">neuron_text_encoder</span> <span class="o">=</span> <span class="n">text_encoder</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">.</span><span class="n">config</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">.</span><span class="n">device</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">neuron_text_encoder</span><span class="p">(</span><span class="n">emb</span><span class="p">)[</span><span class="s">'last_hidden_state'</span><span class="p">]]</span>

<span class="k">class</span> <span class="nc">NeuronSafetyModelWrap</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">safety_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">safety_model</span> <span class="o">=</span> <span class="n">safety_model</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clip_inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">safety_model</span><span class="p">(</span><span class="n">clip_inputs</span><span class="p">).</span><span class="n">values</span><span class="p">())</span>

<span class="c1"># --- Compile CLIP text encoder and save ---
</span><span class="k">print</span><span class="p">(</span><span class="s">"Load model...."</span><span class="p">)</span>
<span class="c1"># Only keep the model being compiled in RAM to minimze memory pressure
</span><span class="n">pipe</span> <span class="o">=</span> <span class="n">StableDiffusionPipeline</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Compile CLIP text encoder and save"</span><span class="p">)</span>
<span class="n">text_encoder</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">text_encoder</span><span class="p">)</span>

<span class="c1"># Apply the wrapper to deal with custom return type
</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">NeuronTextEncoder</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">)</span>

<span class="c1"># Compile text encoder
# This is used for indexing a lookup table in torch.nn.Embedding,
# so using random numbers may give errors (out of range).
</span><span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">49406</span><span class="p">,</span> <span class="mi">18376</span><span class="p">,</span>   <span class="mi">525</span><span class="p">,</span>  <span class="mi">7496</span><span class="p">,</span> <span class="mi">49407</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">]])</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">text_encoder_neuron</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span>
            <span class="n">text_encoder</span><span class="p">.</span><span class="n">neuron_text_encoder</span><span class="p">,</span>
            <span class="n">emb</span><span class="p">,</span>
            <span class="n">compiler_workdir</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'text_encoder'</span><span class="p">),</span>
            <span class="n">compiler_args</span><span class="o">=</span><span class="p">[</span><span class="s">"--enable-fast-loading-neuron-binaries"</span><span class="p">]</span>
            <span class="p">)</span>

<span class="c1"># Save the compiled text encoder
</span><span class="n">text_encoder_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'text_encoder/model.pt'</span><span class="p">)</span>
<span class="n">torch_neuronx</span><span class="p">.</span><span class="n">async_load</span><span class="p">(</span><span class="n">text_encoder_neuron</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">text_encoder_neuron</span><span class="p">,</span> <span class="n">text_encoder_filename</span><span class="p">)</span>

<span class="c1"># delete unused objects
</span><span class="k">del</span> <span class="n">text_encoder</span>
<span class="k">del</span> <span class="n">text_encoder_neuron</span>
<span class="k">del</span> <span class="n">emb</span>

<span class="c1"># --- Compile VAE decoder and save ---
</span><span class="k">print</span><span class="p">(</span><span class="s">"Compile VAE decoder and save"</span><span class="p">)</span>
<span class="c1"># Only keep the model being compiled in RAM to minimze memory pressure
</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">)</span>

<span class="c1"># # Compile vae decoder
</span><span class="n">decoder_in</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">decoder_neuron</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span>
        <span class="n">decoder</span><span class="p">,</span>
        <span class="n">decoder_in</span><span class="p">,</span>
        <span class="n">compiler_workdir</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'vae_decoder'</span><span class="p">),</span>
        <span class="n">compiler_args</span><span class="o">=</span><span class="p">[</span><span class="s">"--enable-fast-loading-neuron-binaries"</span><span class="p">]</span>
    <span class="p">)</span>


<span class="c1"># Save the compiled vae decoder #######################
</span><span class="n">decoder_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'vae_decoder/model.pt'</span><span class="p">)</span>
<span class="n">torch_neuronx</span><span class="p">.</span><span class="n">async_load</span><span class="p">(</span><span class="n">decoder_neuron</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">decoder_neuron</span><span class="p">,</span> <span class="n">decoder_filename</span><span class="p">)</span>

<span class="c1"># delete unused objects
</span><span class="k">del</span> <span class="n">decoder</span>
<span class="k">del</span> <span class="n">decoder_in</span>
<span class="k">del</span> <span class="n">decoder_neuron</span>

<span class="c1"># --- Compile UNet and save ---
</span><span class="k">print</span><span class="p">(</span><span class="s">"Compile UNet and save"</span><span class="p">)</span>
<span class="c1"># Replace original cross-attention module with custom cross-attention module for better performance
</span><span class="n">CrossAttention</span><span class="p">.</span><span class="n">get_attention_scores</span> <span class="o">=</span> <span class="n">get_attention_scores</span>
<span class="c1"># Apply double wrapper to deal with custom return type
</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span> <span class="o">=</span> <span class="n">NeuronUNet</span><span class="p">(</span><span class="n">UNetWrap</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span><span class="p">))</span>
<span class="c1"># Only keep the model being compiled in RAM to minimze memory pressure
</span><span class="n">unet</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">unetwrap</span><span class="p">)</span>


<span class="c1"># Compile unet - FP32
</span><span class="n">sample_1b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>
<span class="n">timestep_1b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">999</span><span class="p">).</span><span class="nb">float</span><span class="p">().</span><span class="n">expand</span><span class="p">((</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">encoder_hidden_states_1b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">77</span><span class="p">,</span> <span class="mi">768</span><span class="p">])</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="n">sample_1b</span><span class="p">,</span> <span class="n">timestep_1b</span><span class="p">,</span> <span class="n">encoder_hidden_states_1b</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">unet_neuron</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span>
        <span class="n">unet</span><span class="p">,</span>
        <span class="n">example_inputs</span><span class="p">,</span>
        <span class="n">compiler_workdir</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'unet'</span><span class="p">),</span>
        <span class="n">compiler_args</span><span class="o">=</span><span class="p">[</span><span class="s">"--model-type=unet-inference"</span><span class="p">,</span> <span class="s">"--enable-fast-loading-neuron-binaries"</span><span class="p">]</span>
    <span class="p">)</span>


<span class="c1"># save compiled unet
</span><span class="n">unet_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'unet/model.pt'</span><span class="p">)</span>
<span class="n">torch_neuronx</span><span class="p">.</span><span class="n">async_load</span><span class="p">(</span><span class="n">unet_neuron</span><span class="p">)</span>
<span class="n">torch_neuronx</span><span class="p">.</span><span class="n">lazy_load</span><span class="p">(</span><span class="n">unet_neuron</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">unet_neuron</span><span class="p">,</span> <span class="n">unet_filename</span><span class="p">)</span>

<span class="c1"># delete unused objects
</span><span class="k">del</span> <span class="n">unet</span>
<span class="k">del</span> <span class="n">unet_neuron</span>
<span class="k">del</span> <span class="n">sample_1b</span>
<span class="k">del</span> <span class="n">timestep_1b</span>
<span class="k">del</span> <span class="n">encoder_hidden_states_1b</span>


<span class="c1"># --- Compile VAE post_quant_conv and save ---
</span><span class="k">print</span><span class="p">(</span><span class="s">"Compile VAE post_quant_conv and save"</span><span class="p">)</span>
<span class="c1"># Only keep the model being compiled in RAM to minimze memory pressure
</span><span class="n">post_quant_conv</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">post_quant_conv</span><span class="p">)</span>

<span class="c1"># # # Compile vae post_quant_conv
</span><span class="n">post_quant_conv_in</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">post_quant_conv_neuron</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span>
        <span class="n">post_quant_conv</span><span class="p">,</span>
        <span class="n">post_quant_conv_in</span><span class="p">,</span>
        <span class="n">compiler_workdir</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'vae_post_quant_conv'</span><span class="p">),</span>
        <span class="n">compiler_args</span><span class="o">=</span><span class="p">[</span><span class="s">"--enable-fast-loading-neuron-binaries"</span><span class="p">]</span>
    <span class="p">)</span>


<span class="c1"># # Save the compiled vae post_quant_conv
</span><span class="n">post_quant_conv_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'vae_post_quant_conv/model.pt'</span><span class="p">)</span>
<span class="n">torch_neuronx</span><span class="p">.</span><span class="n">async_load</span><span class="p">(</span><span class="n">post_quant_conv_neuron</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">post_quant_conv_neuron</span><span class="p">,</span> <span class="n">post_quant_conv_filename</span><span class="p">)</span>

<span class="c1"># delete unused objects
</span><span class="k">del</span> <span class="n">post_quant_conv</span>



<span class="c1"># # --- Compile safety checker and save ---
</span><span class="k">print</span><span class="p">(</span><span class="s">"Compile safety checker and save"</span><span class="p">)</span>
<span class="c1"># Only keep the model being compiled in RAM to minimze memory pressure
</span><span class="n">safety_model</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">safety_checker</span><span class="p">.</span><span class="n">vision_model</span><span class="p">)</span>


<span class="n">clip_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">])</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">safety_model_neuron</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span>
        <span class="n">safety_model</span><span class="p">,</span>
        <span class="n">clip_input</span><span class="p">,</span>
        <span class="n">compiler_workdir</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'safety_model_neuron'</span><span class="p">),</span>
        <span class="n">compiler_args</span><span class="o">=</span><span class="p">[</span><span class="s">"--enable-fast-loading-neuron-binaries"</span><span class="p">]</span>
    <span class="p">)</span>

<span class="c1"># # Save the compiled vae post_quant_conv
</span><span class="n">safety_model_neuron_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">'safety_model_neuron/model.pt'</span><span class="p">)</span>
<span class="n">torch_neuronx</span><span class="p">.</span><span class="n">async_load</span><span class="p">(</span><span class="n">safety_model_neuron</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">safety_model_neuron</span><span class="p">,</span> <span class="n">safety_model_neuron_filename</span><span class="p">)</span>

<span class="c1"># delete unused objects
</span><span class="k">del</span> <span class="n">safety_model_neuron</span>

<span class="k">del</span> <span class="n">pipe</span>

</code></pre></div></div>

<ul>
  <li>需要 diffusers 的版本为 0.14.0</li>
  <li>直接使用了 hf 上的 diffusers 模型：<code class="language-plaintext highlighter-rouge">XpucT/Deliberate</code>，使用最新的 diffusers 代码自己转换的 C 站模型会推理失败。具体原因不详。</li>
  <li>上述代码转换后的模型只支持 512 * 512 的图片，这个代码是相关的shape：<code class="language-plaintext highlighter-rouge">torch.randn([1, 4, 64, 64])</code></li>
</ul>

<p>保存文件： python trans_1.5.py 之后，直接运行 <code class="language-plaintext highlighter-rouge">python trans_1.5.py</code>。</p>

<p>此过程将会生产 neuron 的模型，如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>drwxrwxr-x 3 ubuntu ubuntu 4096 Aug 30 02:23 safety_model_neuron
drwxrwxr-x 3 ubuntu ubuntu 4096 Aug 30 02:14 text_encoder
drwxrwxr-x 3 ubuntu ubuntu 4096 Aug 30 02:22 unet
drwxrwxr-x 3 ubuntu ubuntu 4096 Aug 30 02:18 vae_decoder
drwxrwxr-x 3 ubuntu ubuntu 4096 Aug 30 02:22 vae_post_quant_conv
</code></pre></div></div>

<h3 id="推理">推理</h3>

<p>直接上代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 类库，类和方法的定义，参数定义和上面一致，略。。。
</span><span class="k">print</span><span class="p">(</span><span class="s">"加载原始模型"</span><span class="p">)</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">StableDiffusionPipeline</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">safety_checker</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">UniPCMultistepScheduler</span><span class="p">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">scheduler</span><span class="p">.</span><span class="n">config</span><span class="p">)</span>

<span class="n">text_encoder_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"text_encoder/model.pt"</span><span class="p">)</span>
<span class="n">unet_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"unet/model.pt"</span><span class="p">)</span>
<span class="n">decoder_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"vae_decoder/model.pt"</span><span class="p">)</span>
<span class="n">post_quant_conv_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"vae_post_quant_conv/model.pt"</span>
<span class="p">)</span>
<span class="n">safety_model_neuron_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"safety_model_neuron/model.pt"</span>
<span class="p">)</span>


<span class="c1"># Load the compiled UNet onto two neuron cores.
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"加载 Neuro 转换模型"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"加载 unet"</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">unet</span> <span class="o">=</span> <span class="n">NeuronUNet</span><span class="p">(</span><span class="n">UNetWrap</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span><span class="p">))</span>
<span class="n">device_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">unetwrap</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">unet_filename</span><span class="p">),</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">set_dynamic_batching</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="s">"加载 text_encoder"</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">NeuronTextEncoder</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">text_encoder</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">text_encoder</span><span class="p">.</span><span class="n">neuron_text_encoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">text_encoder_filename</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"加载 vae decoder"</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">decoder_filename</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"加载 vae post_quant_conv"</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">post_quant_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">post_quant_conv_filename</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"加载 safety_checker, skipping..."</span><span class="p">)</span>
<span class="c1"># pipe.safety_checker.vision_model = NeuronSafetyModelWrap(torch.jit.load(safety_model_neuron_filename))
</span>
<span class="c1"># Run pipeline
</span><span class="n">prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">"A 24 y.o pretty girl, masterpiece, 8k, highres"</span><span class="p">,</span>
    <span class="s">"a photo of an astronaut riding a horse on mars"</span><span class="p">,</span>
    <span class="s">"sonic on the moon"</span><span class="p">,</span>
    <span class="s">"elvis playing guitar while eating a hotdog"</span><span class="p">,</span>
    <span class="s">"saved by the bell"</span><span class="p">,</span>
    <span class="s">"engineers eating lunch at the opera"</span><span class="p">,</span>
    <span class="s">"panda eating bamboo on a plane"</span><span class="p">,</span>
    <span class="s">"A digital illustration of a steampunk flying machine in the sky with cogs and mechanisms, 4k, detailed, trending in artstation, fantasy vivid colors"</span><span class="p">,</span>
    <span class="s">"kids playing soccer at the FIFA World Cup"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">total_time</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">prompt</span><span class="p">:</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">negative_prompt</span><span class="o">=</span><span class="s">"nsfw, bad finger, lowres"</span><span class="p">,</span> <span class="n">num_inference_steps</span><span class="o">=</span><span class="mi">20</span>
    <span class="p">).</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">total_time</span> <span class="o">=</span> <span class="n">total_time</span> <span class="o">+</span> <span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
    <span class="n">image</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s">"outputs/</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">.webp"</span><span class="p">,</span> <span class="s">"WEBP"</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Average time: "</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">((</span><span class="n">total_time</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)),</span> <span class="mi">2</span><span class="p">),</span> <span class="s">"seconds"</span><span class="p">)</span>

</code></pre></div></div>

<p>推理顺利完成，速度很快。</p>

<h2 id="sdxl">SDXL</h2>

<p>8月28日刚刚更新的新的 neuron 2.13 版本可以支持 SDXL 了。当前 AMI 未放出，所以需要先手工安装依赖包。</p>

<p>具体的依赖包仓库在：<a href="https://pip.repos.neuron.amazonaws.com/">https://pip.repos.neuron.amazonaws.com/</a></p>

<p>比如安装最新版本的 neuronx-distributed 命令：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>neuronx-distributed transformers-neuronx <span class="nt">-i</span> https://pip.repos.neuron.amazonaws.com/
</code></pre></div></div>

<h3 id="模型转换-1">模型转换</h3>

<p>废话不多说，上代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># filename: trans_xl.py 
</span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch_neuronx</span>
<span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">DiffusionPipeline</span><span class="p">,</span> <span class="n">DPMSolverMultistepScheduler</span>
<span class="kn">from</span> <span class="nn">diffusers.models.unet_2d_condition</span> <span class="kn">import</span> <span class="n">UNet2DConditionOutput</span>
<span class="kn">from</span> <span class="nn">diffusers.models.attention_processor</span> <span class="kn">import</span> <span class="n">Attention</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">image</span> <span class="k">as</span> <span class="n">mpimg</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>

<span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>


<span class="n">COMPILER_WORKDIR_ROOT</span> <span class="o">=</span> <span class="s">"/home/ubuntu/models/sdxl"</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s">"stabilityai/stable-diffusion-xl-base-1.0"</span>


<span class="k">def</span> <span class="nf">get_attention_scores_neuron</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">query</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">key</span><span class="p">.</span><span class="n">size</span><span class="p">():</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">custom_badbmm</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">custom_badbmm</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">attention_probs</span>


<span class="k">def</span> <span class="nf">custom_badbmm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">bmm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">scaled</span> <span class="o">=</span> <span class="n">bmm</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="k">return</span> <span class="n">scaled</span>


<span class="k">class</span> <span class="nc">UNetWrap</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unet</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">unet</span> <span class="o">=</span> <span class="n">unet</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">text_embeds</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">time_ids</span><span class="o">=</span><span class="bp">None</span>
    <span class="p">):</span>
        <span class="n">out_tuple</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">unet</span><span class="p">(</span>
            <span class="n">sample</span><span class="p">,</span>
            <span class="n">timestep</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">added_cond_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"text_embeds"</span><span class="p">:</span> <span class="n">text_embeds</span><span class="p">,</span> <span class="s">"time_ids"</span><span class="p">:</span> <span class="n">time_ids</span><span class="p">},</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">out_tuple</span>


<span class="k">class</span> <span class="nc">NeuronUNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unetwrap</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">unetwrap</span> <span class="o">=</span> <span class="n">unetwrap</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">unetwrap</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">config</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">unetwrap</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">in_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">add_embedding</span> <span class="o">=</span> <span class="n">unetwrap</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">add_embedding</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">unetwrap</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">device</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">added_cond_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">unetwrap</span><span class="p">(</span>
            <span class="n">sample</span><span class="p">,</span>
            <span class="n">timestep</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">expand</span><span class="p">((</span><span class="n">sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)),</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">added_cond_kwargs</span><span class="p">[</span><span class="s">"text_embeds"</span><span class="p">],</span>
            <span class="n">added_cond_kwargs</span><span class="p">[</span><span class="s">"time_ids"</span><span class="p">],</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">UNet2DConditionOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">sample</span><span class="p">)</span>


<span class="c1"># --- Compile VAE decoder and save ---
</span>
<span class="c1"># Only keep the model being compiled in RAM to minimze memory pressure
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Load model...."</span><span class="p">)</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="s">"Compile vae decoder...."</span><span class="p">)</span>

<span class="n">decoder</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">)</span>

<span class="c1"># # Compile vae decoder
</span><span class="n">decoder_in</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="n">decoder_neuron</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span>
    <span class="n">decoder</span><span class="p">,</span>
    <span class="n">decoder_in</span><span class="p">,</span>
    <span class="n">compiler_workdir</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"vae_decoder"</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Save the compiled vae decoder
</span><span class="n">decoder_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"vae_decoder/model.pt"</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">decoder_neuron</span><span class="p">,</span> <span class="n">decoder_filename</span><span class="p">)</span>

<span class="c1"># delete unused objects
</span><span class="k">del</span> <span class="n">decoder</span>


<span class="c1"># --- Compile UNet and save ---
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Compile UNet"</span><span class="p">)</span>

<span class="c1"># pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)
</span>
<span class="c1"># Replace original cross-attention module with custom cross-attention module for better performance
</span><span class="n">Attention</span><span class="p">.</span><span class="n">get_attention_scores</span> <span class="o">=</span> <span class="n">get_attention_scores_neuron</span>

<span class="c1"># Apply double wrapper to deal with custom return type
</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span> <span class="o">=</span> <span class="n">NeuronUNet</span><span class="p">(</span><span class="n">UNetWrap</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span><span class="p">))</span>

<span class="c1"># Only keep the model being compiled in RAM to minimze memory pressure
</span><span class="n">unet</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">unetwrap</span><span class="p">)</span>
<span class="c1"># del pipe
</span>
<span class="c1"># Compile unet - FP32
</span><span class="n">sample_1b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="n">timestep_1b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">999</span><span class="p">).</span><span class="nb">float</span><span class="p">().</span><span class="n">expand</span><span class="p">((</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">encoder_hidden_states_1b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">77</span><span class="p">,</span> <span class="mi">2048</span><span class="p">])</span>
<span class="n">added_cond_kwargs_1b</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"text_embeds"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1280</span><span class="p">]),</span>
    <span class="s">"time_ids"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span>
<span class="p">}</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">sample_1b</span><span class="p">,</span>
    <span class="n">timestep_1b</span><span class="p">,</span>
    <span class="n">encoder_hidden_states_1b</span><span class="p">,</span>
    <span class="n">added_cond_kwargs_1b</span><span class="p">[</span><span class="s">"text_embeds"</span><span class="p">],</span>
    <span class="n">added_cond_kwargs_1b</span><span class="p">[</span><span class="s">"time_ids"</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">unet_neuron</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span>
    <span class="n">unet</span><span class="p">,</span>
    <span class="n">example_inputs</span><span class="p">,</span>
    <span class="n">compiler_workdir</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"unet"</span><span class="p">),</span>
    <span class="n">compiler_args</span><span class="o">=</span><span class="p">[</span><span class="s">"--model-type=unet-inference"</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># save compiled unet
</span><span class="n">unet_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"unet/model.pt"</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">unet_neuron</span><span class="p">,</span> <span class="n">unet_filename</span><span class="p">)</span>

<span class="c1"># delete unused objects
</span><span class="k">del</span> <span class="n">unet</span>


<span class="k">print</span><span class="p">(</span><span class="s">"Compile VAE post_quant_conv "</span><span class="p">)</span>
<span class="c1"># --- Compile VAE post_quant_conv and save ---
</span>
<span class="c1"># Only keep the model being compiled in RAM to minimze memory pressure
# pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)
</span><span class="n">post_quant_conv</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">post_quant_conv</span><span class="p">)</span>
<span class="c1"># del pipe
</span>
<span class="c1"># Compile vae post_quant_conv
</span><span class="n">post_quant_conv_in</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="n">post_quant_conv_neuron</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span>
    <span class="n">post_quant_conv</span><span class="p">,</span>
    <span class="n">post_quant_conv_in</span><span class="p">,</span>
    <span class="n">compiler_workdir</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"vae_post_quant_conv"</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Save the compiled vae post_quant_conv
</span><span class="n">post_quant_conv_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"vae_post_quant_conv/model.pt"</span>
<span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">post_quant_conv_neuron</span><span class="p">,</span> <span class="n">post_quant_conv_filename</span><span class="p">)</span>

<span class="c1"># delete unused objects
</span><span class="k">del</span> <span class="n">post_quant_conv</span>

<span class="k">del</span> <span class="n">pipe</span>

</code></pre></div></div>

<p>但由于 SDXL 的模型要大很多，转换速度比较慢。</p>

<h3 id="推理代码">推理代码</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 头部定义掠过
</span>
<span class="n">decoder_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"vae_decoder/model.pt"</span><span class="p">)</span>
<span class="n">unet_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"unet/model.pt"</span><span class="p">)</span>
<span class="n">post_quant_conv_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">COMPILER_WORKDIR_ROOT</span><span class="p">,</span> <span class="s">"vae_post_quant_conv/model.pt"</span>
<span class="p">)</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">DPMSolverMultistepScheduler</span><span class="p">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">scheduler</span><span class="p">.</span><span class="n">config</span><span class="p">)</span>


<span class="c1"># Load the compiled UNet onto two neuron cores.
</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span> <span class="o">=</span> <span class="n">NeuronUNet</span><span class="p">(</span><span class="n">UNetWrap</span><span class="p">(</span><span class="n">pipe</span><span class="p">.</span><span class="n">unet</span><span class="p">))</span>
<span class="n">device_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">unetwrap</span> <span class="o">=</span> <span class="n">torch_neuronx</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">unet_filename</span><span class="p">),</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">set_dynamic_batching</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="c1"># Load other compiled models onto a single neuron core.
</span><span class="n">pipe</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">decoder_filename</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">post_quant_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">post_quant_conv_filename</span><span class="p">)</span>

<span class="c1"># Run pipeline
</span><span class="n">prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">"A 24 y.o pretty girl, masterpiece, 8k, highres"</span><span class="p">,</span>
    <span class="s">"a photo of an astronaut riding a horse on mars"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">total_time</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">prompt</span><span class="p">:</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">pmt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""anime artwork </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s"> . anime style, key visual, vibrant, studio anime, highly detailed"""</span>
    <span class="n">npmt</span> <span class="o">=</span> <span class="s">"""photo, deformed, black and white, realism, disfigured, low contrast"""</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">pmt</span><span class="p">,</span> <span class="n">negative_prompt</span><span class="o">=</span><span class="n">npmt</span><span class="p">,</span> <span class="n">num_inference_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">).</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">total_time</span> <span class="o">=</span> <span class="n">total_time</span> <span class="o">+</span> <span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
    <span class="n">image</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s">"outputs/xl-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">.webp"</span><span class="p">,</span> <span class="s">"WEBP"</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Average time: "</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">((</span><span class="n">total_time</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)),</span> <span class="mi">2</span><span class="p">),</span> <span class="s">"seconds"</span><span class="p">)</span>

</code></pre></div></div>

<p>过程和 1.5 差不多。</p>

<p>但模型加载的时间较长。</p>

<h2 id="总结">总结</h2>

<ul>
  <li>模型需要转换，基本都能能成功完成转换，</li>
  <li>SD 1.5 的模型能成功完成推理的不多。</li>
  <li>SDXL 模型本身有较高质量，但加载时间较长。</li>
  <li>当前按照官方示例，不支持动图片态尺寸，模型转化的时候已经固定好图片尺寸了。</li>
  <li>推理速度很快：512 尺寸的 SD1.5 可达到 19it/s 左右，1024 的 SDXL 是 3.5it/s。</li>
</ul>

<hr />
<p>参考：</p>

<p><a href="https://github.com/aws-neuron/aws-neuron-samples">https://github.com/aws-neuron/aws-neuron-samples</a></p>

<p><a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html">https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html</a></p>]]></content><author><name>啤酒云</name></author><category term="aws," /><category term="aiml" /><summary type="html"><![CDATA[AWS Inferentia2 实例专为深度学习（DL）推理而构建。它们在 Amazon EC2 中以最低的成本为生成式人工智能（AI）模型（包括大型语言模型（LLM）和视觉转换器）提供高性能计算。您可以使用 Inf2 实例来运行推理应用程序，以实现文本摘要、代码生成、视频和图像生成、语音识别、个性化、欺诈检测等等。]]></summary></entry><entry><title type="html">使用 Cloudfront Lambda@Edge 实现图片格式转换</title><link href="https://youbug.cn/2023/06/cloudfront-image-process-lambda-sam.html" rel="alternate" type="text/html" title="使用 Cloudfront Lambda@Edge 实现图片格式转换" /><published>2023-06-25T10:10:49+00:00</published><updated>2023-06-25T10:10:49+00:00</updated><id>https://youbug.cn/2023/06/cloudfront-image-process-lambda-sam</id><content type="html" xml:base="https://youbug.cn/2023/06/cloudfront-image-process-lambda-sam.html"><![CDATA[<p>Cloudfront Lambda@Edge 可以在边缘端完成一系列逻辑操作。 本文使用 AWS Serverless Application Model(AWS SAM) 实现了将图片处理成 WebP 格式。</p>

<h2 id="应用场景">应用场景</h2>

<p>本示例描述了如下场景：</p>

<ul>
  <li>Cloudfront 的图片路径为： <code class="language-plaintext highlighter-rouge">/{bucket_bucket}/{s3_key}</code></li>
  <li>客户端的 headers 里如果包含 Accept 为 image/webp，则处理图片为 webp 格式</li>
  <li>如果不满足上述条件，则重写 Url 为 <code class="language-plaintext highlighter-rouge">/{s3_key}</code>，直接将 request 导向相应的存储桶</li>
</ul>

<h2 id="sam-应用开发过程">SAM 应用开发过程</h2>

<h3 id="创建应用">创建应用</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sam init
</code></pre></div></div>

<p>一些选项：</p>

<ul>
  <li>选择 python3.9, ZIP （这是默认的）</li>
  <li>hello_world 模版即可</li>
</ul>

<p>完成之后，SAM 会建立一个项目模板。进入项目目录。</p>

<p>我本地的项目名称为 <code class="language-plaintext highlighter-rouge">webp-processor</code>，SAM 系统会以此名字创建一系列资源。</p>

<h3 id="修改配置">修改配置</h3>

<p>由于我们无需 API Gateway 触发器，可以删除项目根目录下 <code class="language-plaintext highlighter-rouge">template.yaml</code> 的关于触发器内容，并增大 Timeout 的时间，如下：</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">Globals</span><span class="pi">:</span>
  <span class="na">Function</span><span class="pi">:</span>
    <span class="na">Timeout</span><span class="pi">:</span> <span class="m">30</span>
<span class="na">Resources</span><span class="pi">:</span>
  <span class="na">CloudfrontFunction</span><span class="pi">:</span>
    <span class="na">Properties</span><span class="pi">:</span>
      <span class="c1"># Events:</span>
      <span class="c1">#   HelloWorld:</span>
      <span class="c1">#     Type: Api # More info about API Event Source: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#api</span>
      <span class="c1">#     Properties:</span>
      <span class="c1">#       Path: /hello</span>
      <span class="c1">#       Method: get</span>
<span class="na">Outputs</span><span class="pi">:</span>
  <span class="c1"># HelloWorldApi:</span>
  <span class="c1">#   Description: "API Gateway endpoint URL for Prod stage for Hello World function"</span>
  <span class="c1">#   Value: !Sub "https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/"</span>
</code></pre></div></div>

<h3 id="代码解读">代码解读</h3>

<blockquote>
  <p>完整的代码请参见文末。</p>
</blockquote>

<p>如果检测到无需处理的请求，直接返回 request，并重写 URL:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">request</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s">"Records"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">"cf"</span><span class="p">][</span><span class="s">"request"</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">uri</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".jpg"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">uri</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".png"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">uri</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".jpeg"</span><span class="p">)):</span> <span class="c1"># 只处理这三种类型的图片
</span>      <span class="n">request</span><span class="p">[</span><span class="s">"uri"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">key</span>
      <span class="k">return</span> <span class="n">request</span>
</code></pre></div></div>

<p>cf 请求过来的 headers 构造比较特殊，如下的示例代码可以方便解析并获取 headers 里的值：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">webpAccept</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">for</span> <span class="n">accept</span> <span class="ow">in</span> <span class="n">headers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'accept'</span><span class="p">,</span> <span class="p">[]):</span>
        <span class="k">if</span> <span class="s">"image/webp"</span> <span class="ow">in</span> <span class="n">accept</span><span class="p">[</span><span class="s">'value'</span><span class="p">]:</span>
            <span class="n">webpAccept</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">break</span>
</code></pre></div></div>

<p>本示例中，需要用到 Pillow 库，直接在 <code class="language-plaintext highlighter-rouge">requirements.txt</code> 里增加依赖即可。</p>

<h3 id="本地调试">本地调试</h3>

<p>使用如下命令，先 build 再调用：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sam build

sam <span class="nb">local </span>invoke <span class="nt">-e</span>  ./events/cf.json 
</code></pre></div></div>

<p>后面这个 json 文件是模拟相应的触发器请求，具体数据格式可以参考这个网址：</p>

<p><a href="https://github.com/tschoffelen/lambda-sample-events/tree/master/events/aws">https://github.com/tschoffelen/lambda-sample-events/tree/master/events/aws</a></p>

<p>本文测试的 <code class="language-plaintext highlighter-rouge">./events/cf.json</code>  如下：</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"Records"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"cf"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"distributionId"</span><span class="p">:</span><span class="w"> </span><span class="s2">"EXAMPLE"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="nl">"request"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"uri"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/test-bucket/web/xxxx.jpg"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"method"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GET"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"clientIp"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2001:cdba::3257:9652"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"headers"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"host"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
              </span><span class="p">{</span><span class="w">
                </span><span class="nl">"key"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Host"</span><span class="p">,</span><span class="w">
                </span><span class="nl">"value"</span><span class="p">:</span><span class="w"> </span><span class="s2">"d123.cf.net"</span><span class="w">
              </span><span class="p">}</span><span class="w">
            </span><span class="p">],</span><span class="w">
            </span><span class="nl">"accept"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
              </span><span class="p">{</span><span class="w">
                </span><span class="nl">"key"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Accept"</span><span class="p">,</span><span class="w">
                </span><span class="nl">"value"</span><span class="p">:</span><span class="w"> </span><span class="s2">"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"</span><span class="w">
              </span><span class="p">}</span><span class="w">
            </span><span class="p">]</span><span class="w">
          </span><span class="p">}</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h3 id="云上调试api-gateway">云上调试（API Gateway）</h3>

<p>如果使用了  API Gateway 作为触发器，那么使用云上调试会比较简单：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sam <span class="nb">sync</span> <span class="nt">--watch</span>
</code></pre></div></div>

<p>本地调试使用下面的命令：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sam <span class="nb">local </span>start-api
</code></pre></div></div>

<p>部署更新代码之前，需要调用 <code class="language-plaintext highlighter-rouge">sam build</code>。</p>

<h2 id="部署">部署</h2>

<h3 id="本地代码部署到线上">本地代码部署到线上</h3>

<p>测试完成之后，调用如下命令进行部署：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sam deploy <span class="nt">--guided</span>
</code></pre></div></div>

<p>按照提示即可将 Lambda 部署到线上。部署完成后可以在 Lambda 控制台看到 <code class="language-plaintext highlighter-rouge">webp-processor-CloudfrontFunction-xxxxx</code> 的 Lambda 实例。</p>

<h3 id="设置-cloudfront-的缓存策略">设置 Cloudfront 的缓存策略</h3>

<p>由于此场景会通过浏览器的 headers 的 Accept 的值来决定是否处理图片，所以需要设置依据 headers 的  Accept 进行缓存。</p>

<p><a href="https://console.aws.amazon.com/cloudfront/v3/home#/policies/cache">进入 Cloudfront 控制台的 Policies</a>，在 Custom policies 面板点击 <code class="language-plaintext highlighter-rouge">Create cache policy</code>。</p>

<p>参数如下：</p>

<ul>
  <li>Name: ForWebP （或者其他任意名字）</li>
  <li>Headers: Include the following headers
    <ul>
      <li>Add Header: Accept</li>
    </ul>
  </li>
</ul>

<p>其余默认即可。</p>

<p>进入 Cloudfront 的 Distributions 实例，选择相应的实例，并设置 Behaviors 的 Cache policy 为 <code class="language-plaintext highlighter-rouge">ForWebP</code>。</p>

<h3 id="部署-lambdaedge">部署 Lambda@Edge</h3>

<p><a href="https://console.aws.amazon.com/lambda/home#/functions">进入 Lambda 控制台</a>，进入刚刚部署的函数 <code class="language-plaintext highlighter-rouge">webp-processor-CloudfrontFunction-xxxxx</code>。</p>

<p>右上角选择 Action -&gt; Deploy to Lambda@Edge，如图：</p>

<p><img src="/assets/posts/aws/deploy-lambda-edge.png" alt="deploy lambda to edge" /></p>

<p>第一次部署选择：</p>

<ul>
  <li>Distribution: 对应的 Cloudfront 实例</li>
  <li>Cache behavior: *</li>
  <li>CloudFront event: Origin requset</li>
  <li>Confirm deploy to Lambda@Edge</li>
</ul>

<p>此处 CloudFront event 只能选择 Origin requset， Viewer Request 只支持小于 1M 的包。</p>

<p>后续升级部署选择 <code class="language-plaintext highlighter-rouge">Use existing CloudFront trigger on this function</code> 即可。</p>

<h2 id="坑">坑</h2>

<h3 id="本示例的特殊之处">本示例的特殊之处</h3>

<p>本示例使用了一个 Cloudfront 映射多个 S3 存储桶，设置 Origins 的时候，需要设置多个 S3 存储桶，请注意这里的 <strong>Origin path 需要留空</strong>。</p>

<p>同时需要设置多个 Behaviors，规则为： Path 是桶名称，对应到 S3 的相应的桶。</p>

<h3 id="登录-public-ecr">登录 public ecr</h3>

<p>在执行 <code class="language-plaintext highlighter-rouge">sam local invoke</code> 的时候，需要提示 docker login 登录。使用如下命令：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws ecr-public get-login-password <span class="nt">--region</span> us-east-1 | docker login <span class="nt">--username</span> AWS <span class="nt">--password-stdin</span> public.ecr.aws/lambda/python
</code></pre></div></div>

<p>之后调试的过程中，如果出现 Error: Unknown API error received from docker 的错误，大概率也是需要执行此命令重新登录。</p>

<h3 id="设置-cloudfront-edge-的权限">设置 Cloudfront@ edge 的权限</h3>

<p>需要编辑 lambda 的执行 role 的 信任关系，加入 <code class="language-plaintext highlighter-rouge">edgelambda.amazonaws.com</code> 这个 Principal。</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
 </span><span class="nl">"Version"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2012-10-17"</span><span class="p">,</span><span class="w">
 </span><span class="nl">"Statement"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
  </span><span class="p">{</span><span class="w">
   </span><span class="nl">"Effect"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Allow"</span><span class="p">,</span><span class="w">
   </span><span class="nl">"Principal"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"Service"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"lambda.amazonaws.com"</span><span class="p">,</span><span class="w"> </span><span class="s2">"edgelambda.amazonaws.com"</span><span class="p">]</span><span class="w">
   </span><span class="p">},</span><span class="w">
   </span><span class="nl">"Action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sts:AssumeRole"</span><span class="w">
  </span><span class="p">}</span><span class="w">
 </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h3 id="设置-lambda-执行权限">设置 Lambda 执行权限</h3>

<p>找到 Lambda 的执行角色，加入对应 S3 Bucket 的读取权限。</p>

<h3 id="限制条件">限制条件</h3>

<p>（到当前为止： 2023-6-26）：</p>

<ul>
  <li>
    <p>&lt;= python 3.9</p>
  </li>
  <li>
    <p>edged 端不支持 arm 架构</p>
  </li>
</ul>

<p>更多限制条件参见：<a href="https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/edge-functions-restrictions.html">https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/edge-functions-restrictions.html</a></p>

<h2 id="完整代码附上">完整代码附上</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">import</span> <span class="nn">base64</span>

<span class="n">s3_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="s">"s3"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">lambda_handler</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
    
    <span class="n">request</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s">"Records"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">"cf"</span><span class="p">][</span><span class="s">"request"</span><span class="p">]</span>
    <span class="n">uri</span> <span class="o">=</span> <span class="n">request</span><span class="p">[</span><span class="s">"uri"</span><span class="p">]</span>  <span class="c1"># 格式：/bucket/key
</span>    <span class="n">headers</span> <span class="o">=</span> <span class="n">request</span><span class="p">[</span><span class="s">"headers"</span><span class="p">]</span>
    
    <span class="n">bucket</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">parse_url</span><span class="p">(</span><span class="n">uri</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">uri</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".jpg"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">uri</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".png"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">uri</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".jpeg"</span><span class="p">)):</span> <span class="c1"># 只处理这三种类型的图片
</span>        <span class="n">request</span><span class="p">[</span><span class="s">"uri"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">key</span>
        <span class="k">return</span> <span class="n">request</span>
        
    <span class="n">webpAccept</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">for</span> <span class="n">accept</span> <span class="ow">in</span> <span class="n">headers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'accept'</span><span class="p">,</span> <span class="p">[]):</span>
        <span class="k">if</span> <span class="s">"image/webp"</span> <span class="ow">in</span> <span class="n">accept</span><span class="p">[</span><span class="s">'value'</span><span class="p">]:</span>
            <span class="n">webpAccept</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">break</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">webpAccept</span><span class="p">:</span>
        <span class="n">request</span><span class="p">[</span><span class="s">"uri"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">key</span>
        <span class="k">return</span> <span class="n">request</span>
    
    <span class="n">webp_image</span> <span class="o">=</span> <span class="n">resize_s3_image</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s">"status"</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
        <span class="s">"bodyEncoding"</span><span class="p">:</span> <span class="s">"base64"</span><span class="p">,</span>
        <span class="s">"headers"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"content-type"</span><span class="p">:</span> <span class="p">[{</span><span class="s">"key"</span><span class="p">:</span> <span class="s">"Content-Type"</span><span class="p">,</span> <span class="s">"value"</span><span class="p">:</span> <span class="s">"image/webp"</span><span class="p">}],</span>
            <span class="s">"content-encoding"</span><span class="p">:</span> <span class="p">[{</span><span class="s">"key"</span><span class="p">:</span> <span class="s">"Content-Encoding"</span><span class="p">,</span> <span class="s">"value"</span><span class="p">:</span> <span class="s">"base64"</span><span class="p">}],</span>
        <span class="p">},</span>
        <span class="s">"body"</span><span class="p">:</span> <span class="n">base64</span><span class="p">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">webp_image</span><span class="p">),</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">parse_url</span><span class="p">(</span><span class="n">uri</span><span class="p">:</span><span class="nb">str</span><span class="p">):</span>
    <span class="n">uri</span> <span class="o">=</span> <span class="n">uri</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">uris</span> <span class="o">=</span> <span class="n">uri</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">"/"</span><span class="p">)</span>
    <span class="n">bucket</span><span class="o">=</span><span class="n">uris</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">uri</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:]</span>



<span class="k">def</span> <span class="nf">resize_s3_image</span><span class="p">(</span><span class="n">bucket_name</span><span class="p">,</span> <span class="n">objectKey</span><span class="p">):</span>
    <span class="n">s3</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">resource</span><span class="p">(</span><span class="s">"s3"</span><span class="p">)</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="n">s3</span><span class="p">.</span><span class="n">Object</span><span class="p">(</span>
        <span class="n">bucket_name</span><span class="o">=</span><span class="n">bucket_name</span><span class="p">,</span>
        <span class="n">key</span><span class="o">=</span><span class="n">objectKey</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">obj_body</span> <span class="o">=</span> <span class="n">obj</span><span class="p">.</span><span class="n">get</span><span class="p">()[</span><span class="s">"Body"</span><span class="p">].</span><span class="n">read</span><span class="p">()</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">obj_body</span><span class="p">))</span>

    <span class="c1"># (w, h) = img.size
</span>    <span class="c1"># img = img.resize((int(w / 8), int(h / 8)), Image.LANCZOS)
</span>
    <span class="nb">buffer</span> <span class="o">=</span> <span class="n">BytesIO</span><span class="p">()</span>
    <span class="n">img</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="nb">buffer</span><span class="p">,</span> <span class="s">"webp"</span><span class="p">)</span>
    <span class="nb">buffer</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">img</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">return</span> <span class="nb">buffer</span><span class="p">.</span><span class="n">getvalue</span><span class="p">()</span>

</code></pre></div></div>]]></content><author><name>啤酒云</name></author><category term="aws" /><summary type="html"><![CDATA[Cloudfront Lambda@Edge 可以在边缘端完成一系列逻辑操作。 本文使用 AWS Serverless Application Model(AWS SAM) 实现了将图片处理成 WebP 格式。]]></summary></entry><entry><title type="html">使用 SageMaker 部署 ChatGLM-6B 自定义 API</title><link href="https://youbug.cn/2023/05/sagemaker-chatglm.html" rel="alternate" type="text/html" title="使用 SageMaker 部署 ChatGLM-6B 自定义 API" /><published>2023-05-28T02:13:33+00:00</published><updated>2023-05-28T02:13:33+00:00</updated><id>https://youbug.cn/2023/05/sagemaker-chatglm</id><content type="html" xml:base="https://youbug.cn/2023/05/sagemaker-chatglm.html"><![CDATA[<p>ChatGLM-6B 默认是一个聊天模型，也可以用来提取 embeddings。但当前的企业内部智能搜索方案大多都使用了 text2vec + LLM 多个模型，text2vec 用于向量生产，LLM 用于对查询结果进行总结。本文试试图使用同一个 LLM 模型完成这两项工作，编写自定义 API，并将模型部署到 SageMaker 上。</p>

<h2 id="关键代码">关键代码</h2>

<p>废话不多说，ChatGLM-6B 抽取 embeddings 的关键代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">to_embeddings</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">text</span><span class="p">):</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
    <span class="n">model_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">model_output</span><span class="p">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
</code></pre></div></div>

<p>上述代码不做过多解释，因为我也不太懂。</p>

<p>生成的结果是一个长度为 4096 的浮点数组。</p>

<h2 id="sagemaker-predict-接口设计">SageMaker predict 接口设计</h2>

<p>也直接看代码吧：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">predict_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pipe</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"text"</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="nb">type</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"type"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">response</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">.</span><span class="n">chat</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">history</span><span class="o">=</span><span class="p">[])</span>
        <span class="k">return</span> <span class="n">response</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">to_embeddings</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
</code></pre></div></div>

<p>预测接口增加了参数 <code class="language-plaintext highlighter-rouge">type</code>，可以通过使用此参数来执行不同的任务，在本例中</p>

<ul>
  <li>type 为 0 执行聊天任务</li>
  <li>else 生成 embeddings</li>
</ul>

<h2 id="部署">部署</h2>

<p>关键代码已经完成，现在只需要将上述逻辑放到 项目的 code 目录下的 <code class="language-plaintext highlighter-rouge">inference.py</code> 文件中，ChatGLM-6B 模型我们直接让 SageMaker 去 Huggingface 上下载。</p>

<p>code 目录我已经打包上传到 S3。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>  
<span class="kn">from</span> <span class="nn">sagemaker.huggingface.model</span> <span class="kn">import</span> <span class="n">HuggingFaceModel</span>

<span class="n">s3_model</span> <span class="o">=</span> <span class="s">"s3://cloudbeer-llm-models/llm/chatglm-6b-model.tar.gz"</span>

<span class="n">iam_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="s">'iam'</span><span class="p">)</span>
<span class="n">role</span> <span class="o">=</span> <span class="n">iam_client</span><span class="p">.</span><span class="n">get_role</span><span class="p">(</span><span class="n">RoleName</span><span class="o">=</span><span class="s">'HuggingfaceExecuteRole'</span><span class="p">)[</span><span class="s">'Role'</span><span class="p">][</span><span class="s">'Arn'</span><span class="p">]</span>

<span class="n">huggingface_model</span> <span class="o">=</span> <span class="n">HuggingFaceModel</span><span class="p">(</span>
  <span class="n">model_data</span><span class="o">=</span><span class="n">s3_model</span><span class="p">,</span>
  <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>
  <span class="n">transformers_version</span><span class="o">=</span><span class="s">'4.26'</span><span class="p">,</span>
  <span class="n">pytorch_version</span><span class="o">=</span><span class="s">'1.13'</span><span class="p">,</span>
  <span class="n">py_version</span><span class="o">=</span><span class="s">'py39'</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">huggingface_model</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
  <span class="n">initial_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.g4dn.2xlarge'</span><span class="p">,</span>
  <span class="n">endpoint_name</span><span class="o">=</span><span class="s">'chatglm-6b-model'</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="测试">测试</h2>

<h4 id="sagemaker-模型加载">SageMaker 模型加载</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sagemaker.huggingface.model</span> <span class="kn">import</span> <span class="n">HuggingFacePredictor</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">HuggingFacePredictor</span><span class="p">(</span>
  <span class="n">endpoint_name</span><span class="o">=</span><span class="s">'chatglm-6b-model'</span>
<span class="p">)</span>
</code></pre></div></div>

<h4 id="对话">对话</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">({</span>
    <span class="s">"text"</span><span class="p">:</span> <span class="s">"你好，你是谁"</span>
<span class="p">})</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">'我是一个名为 ChatGLM-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。'
</code></p>

<h4 id="生产-embedddings">生产 embedddings</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">res</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">({</span>
    <span class="s">"text"</span><span class="p">:</span> <span class="s">"你好世界"</span><span class="p">,</span>
    <span class="s">"type"</span><span class="p">:</span> <span class="mi">1</span>
<span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">res</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">4096 [-0.0092010498046875, 0.0296630859375]</code></p>

<h2 id="避坑">避坑</h2>

<p>下面的坑截止到本文写作日期存在：</p>

<ul>
  <li>ChatGLM-6B 的最小机型应该是 2xlarge，如 ml.g4dn.2xlarge，我使用 xlarge 一直出错，日志显示 模型无法加载到 100%。</li>
  <li>transfomers 的最小版本需求为 4.27.1，当前 SageMaker 的 Huggingface 最高版本是 4.26.1，在 requirements.txt 中加入相应的版本依赖即可。</li>
  <li>如果使用了 CUDA，按照报错信息，需要增加 cpm-kernels 的依赖。</li>
</ul>

<p>我的 code/requirements.txt 如下：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cpm-kernels==1.0.11
transformers==4.27.1
</code></pre></div></div>

<hr />
<p>参考：</p>

<p><a href="https://github.com/cloudbeer/chatglm-infer-sagemaker">本文完整源码</a></p>

<p><a href="https://huggingface.co/THUDM/chatglm-6b">https://huggingface.co/THUDM/chatglm-6b</a></p>

<p><a href="https://github.com/lm-sys/FastChat/blob/51ed4fab89f61988e8395a3268595f1effb8528f/fastchat/serve/model_worker.py#L246">FastChat 中各 LLM 获取 embeddings 的方法</a></p>]]></content><author><name>啤酒云</name></author><category term="aiml," /><category term="sagemaker" /><summary type="html"><![CDATA[ChatGLM-6B 默认是一个聊天模型，也可以用来提取 embeddings。但当前的企业内部智能搜索方案大多都使用了 text2vec + LLM 多个模型，text2vec 用于向量生产，LLM 用于对查询结果进行总结。本文试试图使用同一个 LLM 模型完成这两项工作，编写自定义 API，并将模型部署到 SageMaker 上。]]></summary></entry><entry><title type="html">在 AWS 上使用 Stable Diffusion 给商品更换模特(二)</title><link href="https://youbug.cn/2023/05/sagemaker-sd-inpaint-2.html" rel="alternate" type="text/html" title="在 AWS 上使用 Stable Diffusion 给商品更换模特(二)" /><published>2023-05-16T09:10:49+00:00</published><updated>2023-05-16T09:10:49+00:00</updated><id>https://youbug.cn/2023/05/sagemaker-sd-inpaint-2</id><content type="html" xml:base="https://youbug.cn/2023/05/sagemaker-sd-inpaint-2.html"><![CDATA[<p>现在，我准备上传图片和蒙版到 S3，触发部署代码，并完成任务，推理完成之后，释放 Endpoint。</p>

<h2 id="模型加载流程改善">模型加载流程改善</h2>

<p>在进行试验的过程中，发现每次对模型进行打包和上传 S3 会浪费很多时间。</p>

<p>此处改进一下：<strong>在加载模型的时候，直接从 Huggingface 的 <code class="language-plaintext highlighter-rouge">runwayml/stable-diffusion-inpainting</code> 加载模型</strong>。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">model_dir</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
    <span class="n">pipe</span> <span class="o">=</span> <span class="n">StableDiffusionInpaintPipeline</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="s">"runwayml/stable-diffusion-inpainting"</span><span class="p">,</span>
        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">safety_checker</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span>
    <span class="n">pipe</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pipe</span>
</code></pre></div></div>

<p>使用上面的方法可以从 Huggingface 加载模型，并使用自己的推理代码。</p>

<p>具体做法是：</p>

<ul>
  <li>创建一个文件夹，如 src</li>
  <li>在 src 下创建 code 文件夹</li>
  <li>在 code 文件夹下加入 inference.py 和 requirements.txt 文件</li>
  <li>修改 inference.py 的 model_fn 方法，如上</li>
  <li>打包上传 code 下面的两个文件</li>
</ul>

<p>现在无需下载模型到本地打包了，真实部署的时候，SageMaker 会去下载模型。</p>

<p>当前上传的程序包只有 1k 左右，如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>~/environment/stable-diffusion-inpainting <span class="o">(</span>main<span class="o">)</span> <span class="nv">$ </span>tree
<span class="nb">.</span>
├── code
│   ├── inference.py
│   └── requirements.txt
</code></pre></div></div>

<h2 id="inferencepy-代码解读">inference.py 代码解读</h2>

<p>商品图片下载方式修改：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split_s3_path</span><span class="p">(</span><span class="n">s3_path</span><span class="p">):</span>
    <span class="n">path_parts</span><span class="o">=</span><span class="n">s3_path</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">"s3://"</span><span class="p">,</span><span class="s">""</span><span class="p">).</span><span class="n">split</span><span class="p">(</span><span class="s">"/"</span><span class="p">)</span>
    <span class="n">bucket</span><span class="o">=</span><span class="n">path_parts</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">key</span><span class="o">=</span><span class="s">"/"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">path_parts</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">key</span>

<span class="k">def</span> <span class="nf">download_image</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="n">o</span> <span class="o">=</span> <span class="n">split_s3_path</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">s3</span><span class="p">.</span><span class="n">get_object</span><span class="p">(</span><span class="n">Bucket</span><span class="o">=</span><span class="n">o</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Key</span><span class="o">=</span><span class="n">o</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="s">'Body'</span><span class="p">].</span><span class="n">read</span><span class="p">()</span>
    <span class="n">res_img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">response</span><span class="p">)).</span><span class="n">convert</span><span class="p">(</span><span class="s">"RGB"</span><span class="p">)</span>
    <span class="n">res_img</span> <span class="o">=</span> <span class="n">res_img</span><span class="p">.</span><span class="n">resize</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res_img</span>
</code></pre></div></div>

<ul>
  <li>商品和图片的入参会修改为 S3 地址，如：s3://cloudbeer-llm-models/works/2023-05-16/shirt01.png</li>
  <li>在后期可以做相关触发器，直接读取 S3 地址</li>
</ul>

<p>整个生产流程也进行了改进：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gen</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pipe</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"prompt"</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">image_url</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"image_url"</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">mask_url</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"mask_url"</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"width"</span><span class="p">,</span> <span class="mi">384</span><span class="p">)</span>
    <span class="n">height</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"height"</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    
    <span class="n">image_ori</span> <span class="o">=</span> <span class="n">download_image</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask_url</span><span class="p">:</span> 
        <span class="n">mask_image</span> <span class="o">=</span> <span class="n">download_image</span><span class="p">(</span><span class="n">mask_url</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mask_url</span> <span class="o">=</span> <span class="n">image_url</span>
        <span class="n">mask_image</span> <span class="o">=</span> <span class="n">image_ori</span>

    <span class="n">num_inference_steps</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"num_inference_steps"</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">guidance_scale</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"guidance_scale"</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="n">num_images_per_prompt</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"num_images_per_prompt"</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">prompt_suffix</span> <span class="o">=</span> <span class="s">",fine skin,masterpiece,cinematic light, ultra high res, film grain, perfect anatomy, best shadow, delicate,(photorealistic:1.4),(extremely intricate:1.2)"</span>
    <span class="n">nprompt</span> <span class="o">=</span> <span class="s">'bad_legs,bad_fingers,(semi_realistic,cgi,3d,render,sketch,cartoon,drawing,anime:1.4),text,cropped,out_of_frame,worst_quality,low_quality,jpeg_artifacts,ugly,duplicate,morbid,mutilated,extra_fingers,mutated_hands,poorly_drawn_hands,poorly_drawn_face,mutation,deformed,blurry,dehydrated,bad_anatomy,bad_proportions,extra_limbs,cloned_face,disfigured,gross_proportions,malformed_limbs,missing_arms,missing_legs,extra_arms,extra_legs,fused_fingers,too_many_fingers,long_neck,signature'</span>


    <span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">()</span> 
    <span class="n">date_str</span> <span class="o">=</span> <span class="n">now</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y-%m-%d"</span><span class="p">)</span>

    <span class="n">html</span> <span class="o">=</span> <span class="s">"&lt;html&gt;&lt;head&gt;&lt;title&gt;图片生成"</span> <span class="o">+</span> <span class="n">date_str</span> <span class="o">+</span> <span class="s">"&lt;/title&gt;&lt;link href='../main.css' rel='stylesheet'&gt;&lt;/head&gt;&lt;body&gt;"</span>
    <span class="n">html</span> <span class="o">+=</span> <span class="s">"&lt;h1&gt;图片生成"</span> <span class="o">+</span> <span class="n">date_str</span> <span class="o">+</span> <span class="s">"&lt;/h1&gt;"</span>
    <span class="n">html</span> <span class="o">+=</span> <span class="s">"&lt;h4&gt;提示词: "</span> <span class="o">+</span> <span class="n">prompt</span> <span class="o">+</span> <span class="n">prompt_suffix</span> <span class="o">+</span> <span class="s">"&lt;/h4&gt;"</span>
    
    <span class="n">cf_in_url</span> <span class="o">=</span> <span class="n">s3_to_cf_url</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>
    <span class="n">cf_msk_url</span> <span class="o">=</span> <span class="n">s3_to_cf_url</span><span class="p">(</span><span class="n">mask_url</span><span class="p">)</span>
    <span class="n">html</span> <span class="o">+=</span> <span class="s">"&lt;div&gt;&lt;a href='"</span> <span class="o">+</span> <span class="n">cf_in_url</span> <span class="o">+</span> <span class="s">"' target='_blank'&gt;&lt;img src='"</span> <span class="o">+</span> <span class="n">cf_in_url</span> <span class="o">+</span> <span class="s">"' /&gt;&lt;/a&gt;"</span>
    <span class="n">html</span> <span class="o">+=</span> <span class="s">"&lt;a href='"</span> <span class="o">+</span> <span class="n">cf_msk_url</span> <span class="o">+</span> <span class="s">"' target='_blank'&gt;&lt;img src='"</span> <span class="o">+</span> <span class="n">cf_msk_url</span> <span class="o">+</span> <span class="s">"' /&gt;&lt;/a&gt;&lt;/div&gt;"</span>


    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_images_per_prompt</span><span class="p">):</span>
        <span class="n">generated_images</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">prompt_suffix</span><span class="p">,</span>
            <span class="n">negative_prompt</span><span class="o">=</span><span class="n">nprompt</span><span class="p">,</span>
            <span class="n">image</span><span class="o">=</span><span class="n">image_ori</span><span class="p">,</span> 
            <span class="n">mask_image</span><span class="o">=</span><span class="n">mask_image</span><span class="p">,</span>
            <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
            <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
            <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">num_inference_steps</span><span class="p">,</span>
            <span class="n">guidance_scale</span><span class="o">=</span><span class="n">guidance_scale</span><span class="p">,</span>
            <span class="n">num_images_per_prompt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)[</span><span class="s">"images"</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">generated_images</span><span class="p">:</span>
            <span class="n">file_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="p">.</span><span class="n">uuid4</span><span class="p">())</span> <span class="o">+</span> <span class="s">".jpg"</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key_prefix</span> <span class="o">+</span> <span class="n">date_str</span> <span class="o">+</span> <span class="s">'/'</span> <span class="o">+</span> <span class="n">file_name</span>
            <span class="n">in_mem_file</span> <span class="o">=</span> <span class="n">BytesIO</span><span class="p">()</span>
            <span class="n">image</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">in_mem_file</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s">"JPEG"</span><span class="p">)</span>
            <span class="n">in_mem_file</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">s3</span><span class="p">.</span><span class="n">upload_fileobj</span><span class="p">(</span>
                <span class="n">in_mem_file</span><span class="p">,</span> 
                <span class="n">saving_bucket</span><span class="p">,</span> 
                <span class="n">key</span><span class="p">,</span>
                <span class="n">ExtraArgs</span><span class="o">=</span><span class="p">{</span>
                    <span class="s">'ContentType'</span><span class="p">:</span> <span class="s">'image/jpeg'</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="n">html</span> <span class="o">+=</span> <span class="s">"&lt;a href='"</span> <span class="o">+</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s">"' target='_blank'&gt;&lt;img src='"</span> <span class="o">+</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s">"' /&gt;&lt;/a&gt;"</span>
    <span class="n">html</span> <span class="o">+=</span> <span class="s">"&lt;/body&gt;&lt;/html&gt;"</span>

    <span class="n">index_file_name</span> <span class="o">=</span> <span class="s">'index-'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="p">.</span><span class="n">uuid4</span><span class="p">())</span> <span class="o">+</span> <span class="s">'.html'</span>
    <span class="n">s3</span><span class="p">.</span><span class="n">put_object</span><span class="p">(</span>
        <span class="n">Bucket</span><span class="o">=</span><span class="s">'cloudbeer-llm-models'</span><span class="p">,</span>
        <span class="n">Key</span><span class="o">=</span><span class="n">key_prefix</span> <span class="o">+</span> <span class="n">date_str</span> <span class="o">+</span> <span class="s">'/'</span> <span class="o">+</span> <span class="n">index_file_name</span><span class="p">,</span>
        <span class="n">Body</span><span class="o">=</span><span class="n">html</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">),</span>
        <span class="n">ContentType</span><span class="o">=</span><span class="s">'text/html'</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">cloudfront_url</span> <span class="o">+</span> <span class="n">key_prefix</span> <span class="o">+</span> <span class="n">date_str</span> <span class="o">+</span> <span class="s">'/'</span> <span class="o">+</span> <span class="n">index_file_name</span>

</code></pre></div></div>

<p>代码中需要注意的是：</p>

<ul>
  <li>推理 pipeline 中，改为每次生产 1 张图片，num_images_per_prompt 改为循环次数，这样可以有效避免内存溢出</li>
  <li>每生产一张图片，就上传到 S3</li>
  <li>将此次任务的图片生产出一个 html 预览页</li>
  <li>S3 会通过 Cloudfront 映射出来，方便预览</li>
</ul>

<p>代码完成之后，可以直接打包，并上传 S3:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ./src

<span class="nb">rm </span>stable-diffusion-inpainting-tryon.tar.gz
<span class="nb">tar </span>zcvf stable-diffusion-inpainting-tryon.tar.gz <span class="k">*</span>

aws s3 <span class="nb">cp </span>stable-diffusion-inpainting-tryon.tar.gz <span class="se">\</span>
  s3://cloudbeer-llm-models/diffusers/stable-diffusion-inpainting-tryon.tar.gz
</code></pre></div></div>

<h2 id="模型部署">模型部署</h2>

<p>直接通过 SageMaker 部署：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>  
<span class="kn">from</span> <span class="nn">sagemaker.huggingface.model</span> <span class="kn">import</span> <span class="n">HuggingFaceModel</span>

<span class="n">s3_model</span> <span class="o">=</span> <span class="s">"s3://cloudbeer-llm-models/diffusers/stable-diffusion-inpainting-tryon.tar.gz"</span>

<span class="n">iam_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="s">'iam'</span><span class="p">)</span>
<span class="n">role</span> <span class="o">=</span> <span class="n">iam_client</span><span class="p">.</span><span class="n">get_role</span><span class="p">(</span><span class="n">RoleName</span><span class="o">=</span><span class="s">'HuggingfaceExecuteRole'</span><span class="p">)[</span><span class="s">'Role'</span><span class="p">][</span><span class="s">'Arn'</span><span class="p">]</span>

<span class="n">huggingface_model</span> <span class="o">=</span> <span class="n">HuggingFaceModel</span><span class="p">(</span>
  <span class="n">model_data</span><span class="o">=</span><span class="n">s3_model</span><span class="p">,</span>
  <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>
  <span class="n">transformers_version</span><span class="o">=</span><span class="s">"4.26"</span><span class="p">,</span>  
  <span class="n">pytorch_version</span><span class="o">=</span><span class="s">"1.13"</span><span class="p">,</span>
  <span class="n">py_version</span><span class="o">=</span><span class="s">'py39'</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">huggingface_model</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
  <span class="n">initial_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.g4dn.xlarge'</span><span class="p">,</span>
  <span class="n">endpoint_name</span><span class="o">=</span><span class="s">'sd-inpainting-tryon'</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>由于要从 SageMaker 上传 S3，HuggingfaceExecuteRole 这个角色要加入相应 S3 写权限</li>
</ul>

<p>大约 5 分钟后，部署完成。</p>

<h2 id="推理任务">推理任务</h2>

<p>推理代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sagemaker.huggingface.model</span> <span class="kn">import</span> <span class="n">HuggingFacePredictor</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">HuggingFacePredictor</span><span class="p">(</span>
  <span class="n">endpoint_name</span><span class="o">=</span><span class="s">'sd-inpainting-tryon'</span>
<span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">({</span>
    <span class="s">"prompt"</span><span class="p">:</span> <span class="s">"a strong man,back view,white shorts,football field"</span><span class="p">,</span>
    <span class="s">"image_url"</span><span class="p">:</span> <span class="s">"s3://cloudbeer-llm-models/works/2023-05-16/shirt01.png"</span><span class="p">,</span>
    <span class="s">"mask_url"</span><span class="p">:</span> <span class="s">"s3://cloudbeer-llm-models/works/2023-05-16/shirt01_mask.png"</span><span class="p">,</span>
    <span class="s">"num_images_per_prompt"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s">"width"</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s">"height"</span><span class="p">:</span> <span class="mi">512</span>
<span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div>

<p>当前手工执行的，后期可以加入 CloudWatch 或者 S3 时间触发器进行调用。</p>

<p>会打印一个 url，内容大概如下：</p>

<p><img src="/assets/posts/aiml/sd-inpaiting-2.jpg" alt="sd-inpainting-tryon" /></p>

<p>推理任务完成后，删除计算资源：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictor</span><span class="p">.</span><span class="n">delete_model</span><span class="p">()</span>
<span class="n">predictor</span><span class="p">.</span><span class="n">delete_endpoint</span><span class="p">()</span>
</code></pre></div></div>

<hr />

<p>参考：</p>

<p><a href="https://github.com/cloudbeer/sd-inpainting-tryon">本文完整源码</a></p>

<p><a href="sagemaker-sd-inpaint-1.html">在 AWS 上使用 Stable Diffusion 给商品更换模特(一)</a></p>]]></content><author><name>啤酒云</name></author><category term="aiml," /><category term="aws" /><summary type="html"><![CDATA[现在，我准备上传图片和蒙版到 S3，触发部署代码，并完成任务，推理完成之后，释放 Endpoint。]]></summary></entry><entry><title type="html">在 AWS 上使用 Stable Diffusion 给商品更换模特(一)</title><link href="https://youbug.cn/2023/05/sagemaker-sd-inpaint-1.html" rel="alternate" type="text/html" title="在 AWS 上使用 Stable Diffusion 给商品更换模特(一)" /><published>2023-05-14T12:10:49+00:00</published><updated>2023-05-14T12:10:49+00:00</updated><id>https://youbug.cn/2023/05/sagemaker-sd-inpaint-1</id><content type="html" xml:base="https://youbug.cn/2023/05/sagemaker-sd-inpaint-1.html"><![CDATA[<p>给商品图片安上模特可以使用 stable-diffusion-inpainting 这个模型来轻松实现。本文探讨使用 AWS 服务来进行流水线方式生产，并最大程度节约使用成本。</p>

<h2 id="模型准备">模型准备</h2>

<p>建议使用云上实例来操作此步，如 SageMaker 的笔记本实例，或者在相关的 region 里开通 Cloud9。在云上操作，下载和上传速度会更快。</p>

<p>下载 Huggingface 模型：stable-diffusion-inpainting</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git lfs <span class="nb">install
</span>git clone https://huggingface.co/runwayml/stable-diffusion-inpainting
</code></pre></div></div>

<blockquote>
  <p>在云上的小机型使用 git clone 大模型的时候会出现 OOM 错误，上述模型大小为 4G，使用小于 2c4g 的实例 Clone 会 OOM。</p>

  <p>使用如下命令：</p>

  <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git lfs <span class="nb">install</span> <span class="nt">--skip-smudge</span>
git clone https://huggingface.co/runwayml/stable-diffusion-inpainting
<span class="nb">cd </span>stable-diffusion-inpainting
git lfs pull
</code></pre></div>  </div>
</blockquote>

<p>在 SageMaker 中自定义推理，需要编写一个 code 目录，并放上 2 个文件：</p>

<p>code 目录下的 inference.py 文件：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">StableDiffusionInpaintPipeline</span>


<span class="k">def</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">model_dir</span><span class="p">):</span>
    <span class="n">pipe</span> <span class="o">=</span> <span class="n">StableDiffusionInpaintPipeline</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">model_dir</span><span class="p">,</span>
        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">safety_checker</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span>
    <span class="n">pipe</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pipe</span>

<span class="k">def</span> <span class="nf">download_image</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">res_img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">content</span><span class="p">)).</span><span class="n">convert</span><span class="p">(</span><span class="s">"RGB"</span><span class="p">)</span>
    <span class="n">res_img</span> <span class="o">=</span> <span class="n">res_img</span><span class="p">.</span><span class="n">resize</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res_img</span>

<span class="k">def</span> <span class="nf">gen</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pipe</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"prompt"</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">image_url</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"image_url"</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">mask_url</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"mask_url"</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"width"</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    <span class="n">height</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"height"</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    
    <span class="n">image_ori</span> <span class="o">=</span> <span class="n">download_image</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask_url</span><span class="p">:</span> 
        <span class="n">mask_image</span> <span class="o">=</span> <span class="n">download_image</span><span class="p">(</span><span class="n">mask_url</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mask_image</span> <span class="o">=</span> <span class="n">image_ori</span>


    <span class="n">num_inference_steps</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"num_inference_steps"</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">guidance_scale</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"guidance_scale"</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="n">num_images_per_prompt</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"num_images_per_prompt"</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">prompt_suffix</span> <span class="o">=</span> <span class="s">",pretty face,fine skin,masterpiece,cinematic light, ultra high res, film grain, perfect anatomy, best shadow, delicate,(photorealistic:1.4),(extremely intricate:1.2)"</span>
    <span class="n">nprompt</span> <span class="o">=</span> <span class="s">'bad_legs,bad_fingers,(semi_realistic,cgi,3d,render,sketch,cartoon,drawing,anime:1.4),text,cropped,out_of_frame,worst_quality,low_quality,jpeg_artifacts,ugly,duplicate,morbid,mutilated,extra_fingers,mutated_hands,poorly_drawn_hands,poorly_drawn_face,mutation,deformed,blurry,dehydrated,bad_anatomy,bad_proportions,extra_limbs,cloned_face,disfigured,gross_proportions,malformed_limbs,missing_arms,missing_legs,extra_arms,extra_legs,fused_fingers,too_many_fingers,long_neck,signature'</span>

    <span class="n">generated_images</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">prompt_suffix</span><span class="p">,</span>
        <span class="n">negative_prompt</span><span class="o">=</span><span class="n">nprompt</span><span class="p">,</span>
        <span class="n">image</span><span class="o">=</span><span class="n">image_ori</span><span class="p">,</span> 
        <span class="n">mask_image</span><span class="o">=</span><span class="n">mask_image</span><span class="p">,</span>  
        <span class="n">eta</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
        <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
        <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">num_inference_steps</span><span class="p">,</span>
        <span class="n">guidance_scale</span><span class="o">=</span><span class="n">guidance_scale</span><span class="p">,</span>
        <span class="n">num_images_per_prompt</span><span class="o">=</span><span class="n">num_images_per_prompt</span><span class="p">,</span>
    <span class="p">)[</span><span class="s">"images"</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">generated_images</span>

<span class="k">def</span> <span class="nf">predict_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pipe</span><span class="p">):</span>
    <span class="n">generated_images</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pipe</span><span class="p">)</span>
    <span class="n">encoded_images</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">generated_images</span><span class="p">:</span>
        <span class="n">buffered</span> <span class="o">=</span> <span class="n">BytesIO</span><span class="p">()</span>
        <span class="n">image</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">buffered</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s">"JPEG"</span><span class="p">)</span>
        <span class="n">encoded_images</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">base64</span><span class="p">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">buffered</span><span class="p">.</span><span class="n">getvalue</span><span class="p">()).</span><span class="n">decode</span><span class="p">())</span>

    <span class="k">return</span> <span class="p">{</span><span class="s">"generated_images"</span><span class="p">:</span> <span class="n">encoded_images</span><span class="p">}</span>

</code></pre></div></div>

<ul>
  <li>内置定义了部分 prompt  和 完整的 negative_prompt，这些 prompt 可以帮助生产质量比较高的图片，在使用过程中只需要输入你需要的模特提示词即可。</li>
  <li>width 和 height 定义了图片的像素大小，默认 512*512，此尺寸也可以在推理的时候指定，建议不要太大（根据机型的GPU内存不同，太大会溢出），并且尺寸必须是 8 的倍数。</li>
  <li>商品图片的处理建议：
    <ul>
      <li>image_url: 定义了原来的商品图片，此图片最好是纯白底，当前 SD 模型可以直接重绘白色部分，<strong>使用自己作为遮罩 mask</strong>。</li>
      <li>如果你的商品图片是白色基调，那么则需要处理一张遮罩图片，将商品部分涂黑，需要重绘的部分变成白色。</li>
      <li>商品建议不要充满整张图片，需要将模特的头部留出空白位置，如果需要身体其他部分，也需要留出位置。</li>
    </ul>
  </li>
</ul>

<p>code 目录下 requirements.txt：</p>

<pre><code class="language-txt">diffusers[torch]==0.16.1
</code></pre>

<p>现在你的项目目录应该类似这样：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>~/environment/stable-diffusion-inpainting <span class="o">(</span>main<span class="o">)</span> <span class="nv">$ </span>tree
<span class="nb">.</span>
├── code
│   ├── inference.py
│   └── requirements.txt
├── config.json
├── feature_extractor
│   └── preprocessor_config.json
├── model_index.json
├── README.md
├── safety_checker
│   ├── config.json
│   └── pytorch_model.bin
├── scheduler
│   └── scheduler_config.json
├── sd-v1-5-inpainting.ckpt
├── text_encoder
│   ├── config.json
│   └── pytorch_model.bin
├── tokenizer
│   ├── merges.txt
│   ├── special_tokens_map.json
│   ├── tokenizer_config.json
│   └── vocab.json
├── unet
│   ├── config.json
│   └── diffusion_pytorch_model.bin
└── vae
    ├── config.json
    └── diffusion_pytorch_model.bin
</code></pre></div></div>

<p>打包上传 S3:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tar </span>zcvf stable-diffusion-inpainting.tar.gz <span class="k">*</span>

aws s3 <span class="nb">cp </span>stable-diffusion-inpainting.tar.gz <span class="se">\</span>
  s3://cloudbeer-llm-models/diffusers/stable-diffusion-inpainting.tar.gz
</code></pre></div></div>

<blockquote>
  <p>当前模型整体有 8 G，打包上传这一步对于简中用户会比较痛苦，特别是代码写错的情况下。</p>

  <p>我很痛苦！！</p>
</blockquote>

<h2 id="部署模型">部署模型</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>  
<span class="kn">from</span> <span class="nn">sagemaker.huggingface.model</span> <span class="kn">import</span> <span class="n">HuggingFaceModel</span>

<span class="n">s3_model</span> <span class="o">=</span> <span class="s">"s3://cloudbeer-llm-models/diffusers/stable-diffusion-inpainting.tar.gz"</span>

<span class="n">iam_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="s">'iam'</span><span class="p">)</span>
<span class="n">role</span> <span class="o">=</span> <span class="n">iam_client</span><span class="p">.</span><span class="n">get_role</span><span class="p">(</span><span class="n">RoleName</span><span class="o">=</span><span class="s">'HuggingfaceExecuteRole'</span><span class="p">)[</span><span class="s">'Role'</span><span class="p">][</span><span class="s">'Arn'</span><span class="p">]</span>

<span class="n">huggingface_model</span> <span class="o">=</span> <span class="n">HuggingFaceModel</span><span class="p">(</span>
  <span class="n">model_data</span><span class="o">=</span><span class="n">s3_model</span><span class="p">,</span>
  <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>                    
  <span class="n">transformers_version</span><span class="o">=</span><span class="s">"4.26"</span><span class="p">,</span>  
  <span class="n">pytorch_version</span><span class="o">=</span><span class="s">"1.13"</span><span class="p">,</span>
  <span class="n">py_version</span><span class="o">=</span><span class="s">'py39'</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">huggingface_model</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
  <span class="n">initial_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.g4dn.xlarge'</span><span class="p">,</span>
  <span class="n">endpoint_name</span><span class="o">=</span><span class="s">'sd-inpainting-try-on'</span>
<span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>使用了 GPU 机型 ml.g4dn.xlarge</li>
  <li>给 Endpoint 定义了个名称：sd-inpainting-try-on</li>
</ul>

<h2 id="测试推理">测试推理</h2>

<p>下面的测试在本地 Notebook 中进行的，代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sagemaker.huggingface.model</span> <span class="kn">import</span> <span class="n">HuggingFacePredictor</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">HuggingFacePredictor</span><span class="p">(</span>
  <span class="n">endpoint_name</span><span class="o">=</span><span class="s">'sd-inpainting-try-on'</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>


<span class="k">def</span> <span class="nf">display_images</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">image</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)</span> <span class="o">/</span> <span class="n">columns</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">columns</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">decode_base64_image</span><span class="p">(</span><span class="n">image_string</span><span class="p">):</span>
  <span class="n">base64_image</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">image_string</span><span class="p">)</span>
  <span class="nb">buffer</span> <span class="o">=</span> <span class="n">BytesIO</span><span class="p">(</span><span class="n">base64_image</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">({</span>
    <span class="s">"prompt"</span><span class="p">:</span> <span class="s">"a strong man,football field, back view"</span><span class="p">,</span>
    <span class="s">"image_url"</span><span class="p">:</span> <span class="s">"https://d1ffqcflvp9rc.cloudfront.net/samples/images/shirt01.png"</span><span class="p">,</span>
    <span class="s">"mask_url"</span><span class="p">:</span> <span class="s">"https://d1ffqcflvp9rc.cloudfront.net/samples/images/shirt01_mask.png"</span><span class="p">,</span>
    <span class="s">"width"</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s">"height"</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s">"num_images_per_prompt"</span><span class="p">:</span> <span class="mi">2</span>
<span class="p">})</span>

<span class="n">decoded_images</span> <span class="o">=</span> <span class="p">[</span><span class="n">decode_base64_image</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="s">"generated_images"</span><span class="p">]]</span>

<span class="n">display_images</span><span class="p">(</span><span class="n">decoded_images</span><span class="p">)</span>

</code></pre></div></div>

<ul>
  <li>从 Endpoint 的名称 <code class="language-plaintext highlighter-rouge">sd-inpainting-try-on</code> 中获取一个 HuggingFacePredictor 实例</li>
  <li>调用 predict 方法完成推理</li>
</ul>

<p>下面是图片的结果：</p>

<table>
  <thead>
    <tr>
      <th>原图</th>
      <th>蒙版</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://d1ffqcflvp9rc.cloudfront.net/samples/images/shirt01.png" alt="skirt" /></td>
      <td><img src="https://d1ffqcflvp9rc.cloudfront.net/samples/images/shirt01_mask.png" alt="skirt mask" /></td>
    </tr>
  </tbody>
</table>

<p>结果样例</p>

<p><img src="/assets/posts/aiml/sd-inpaiting-skirt-output.jpg" alt="output samples" /></p>

<h2 id="流水线处理">流水线处理</h2>

<p>接下来我想设计一个流水线来处理此过程，让图片可以被批量自动处理，处理结束之后，可以自动结束计算资源。</p>

<p>待续…</p>

<hr />

<p>参考：</p>

<p><a href="sagemaker-sd-inpaint-2.html">在 AWS 上使用 Stable Diffusion 给商品更换模特(二)</a></p>]]></content><author><name>啤酒云</name></author><category term="aiml," /><category term="aws" /><summary type="html"><![CDATA[给商品图片安上模特可以使用 stable-diffusion-inpainting 这个模型来轻松实现。本文探讨使用 AWS 服务来进行流水线方式生产，并最大程度节约使用成本。]]></summary></entry><entry><title type="html">在 SageMaker 上部署 Huggingface 模型 (二)</title><link href="https://youbug.cn/2023/05/deploy-huggingface-model-2-sagemaker-2.html" rel="alternate" type="text/html" title="在 SageMaker 上部署 Huggingface 模型 (二)" /><published>2023-05-11T08:13:33+00:00</published><updated>2023-05-11T08:13:33+00:00</updated><id>https://youbug.cn/2023/05/deploy-huggingface-model-2-sagemaker-2</id><content type="html" xml:base="https://youbug.cn/2023/05/deploy-huggingface-model-2-sagemaker-2.html"><![CDATA[<p>在 Huggingface 上，有些模型并没有 Deploy - Sagemaker 这个功能，或者我们需要做一些特殊的任务，怎么办？本文介绍了如何让 SageMaker 调用自定义的推理代码。</p>

<h2 id="背景">背景</h2>

<p>在 <a href="deploy-huggingface-model-2-sagemaker-1.html">上一篇文章</a> 里，我们直接使用了 Huggingface 官方提供的部署和推理代码中，并使用了他提供了默认的 HF_TASK，如：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hub</span> <span class="o">=</span>  <span class="p">{</span>
 <span class="s">'HF_MODEL_ID'</span><span class="p">:</span><span class="s">'bert-base-chinese'</span><span class="p">,</span>
 <span class="s">'HF_TASK'</span><span class="p">:</span><span class="s">'fill-mask'</span>
<span class="p">}</span>
</code></pre></div></div>

<p>现在我们计划做一个推理 embeddings 文本转向量任务，使用自己的推理代码。</p>

<h2 id="代码准备">代码准备</h2>

<p>本文选取了 distiluse-base-multilingual-cased-v2  这个模型，这个模型可以推理多语言的 embeddings，他在 Huggingface 上没有部署到 SageMaker 的选项。</p>

<p>先下载这个代码仓库：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git lfs <span class="nb">install
</span>git clone https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2
</code></pre></div></div>

<p>进入这个代码仓库，在编辑器里增加两个文件，一个 推理文件 <code class="language-plaintext highlighter-rouge">code/inference.py</code>，一个依赖库文件 <code class="language-plaintext highlighter-rouge">code/requirements.txt</code>。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>distiluse-base-multilingual-cased-v2
<span class="nb">mkdir </span>code
<span class="nb">touch </span>code/inference.py
<span class="nb">touch </span>code/requirements.txt 
</code></pre></div></div>

<p>我们需要在 <code class="language-plaintext highlighter-rouge">inference.py</code> 里添加 model_fn 和 predict_fn 这俩函数，这个文件如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="k">def</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">model_dir</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">predict_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<p>在 <code class="language-plaintext highlighter-rouge">requirements.txt</code> 里加入 sentence_transformers 的引用：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sentence_transformers==2.2.2
</code></pre></div></div>

<h2 id="打包上传">打包上传</h2>

<p>现在把这个库打包上传到 S3：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tar </span>zcvf distiluse-base-multilingual-cased-v2.tar.gz <span class="k">*</span>

aws s3 <span class="nb">cp </span>distiluse-base-multilingual-cased-v2.tar.gz <span class="se">\</span>
  s3://cloudbeer-llm-models/sentence-transformers/distiluse-base-multilingual-cased-v2.tar.gz
</code></pre></div></div>

<ul>
  <li>创建你自己的存储桶，并保证你有足够的权限。</li>
  <li>模型都很大，上传前记得测试一下你的脚本。</li>
</ul>

<h2 id="部署模型">部署模型</h2>

<p>与上一篇差不多，先给 <code class="language-plaintext highlighter-rouge">HuggingfaceExecuteRole</code> 这个 IAM Role 加入策略 <code class="language-plaintext highlighter-rouge">AmazonS3ReadOnlyAccess</code>。</p>

<p>完整的部署代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>  
<span class="kn">from</span> <span class="nn">sagemaker.huggingface.model</span> <span class="kn">import</span> <span class="n">HuggingFaceModel</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cosine</span>

<span class="c1"># 此处定义了一个数组相似度计算的函数
</span><span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">v2</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">-</span><span class="n">cosine</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

<span class="n">s3_model</span> <span class="o">=</span> <span class="s">"s3://cloudbeer-llm-models/sentence-transformers/distiluse-base-multilingual-cased-v2.tar.gz"</span>

<span class="n">iam_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="s">'iam'</span><span class="p">)</span>
<span class="n">role</span> <span class="o">=</span> <span class="n">iam_client</span><span class="p">.</span><span class="n">get_role</span><span class="p">(</span><span class="n">RoleName</span><span class="o">=</span><span class="s">'HuggingfaceExecuteRole'</span><span class="p">)[</span><span class="s">'Role'</span><span class="p">][</span><span class="s">'Arn'</span><span class="p">]</span>

<span class="n">huggingface_model</span> <span class="o">=</span> <span class="n">HuggingFaceModel</span><span class="p">(</span>
  <span class="n">model_data</span><span class="o">=</span><span class="n">s3_model</span><span class="p">,</span>      
  <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>                    
  <span class="n">transformers_version</span><span class="o">=</span><span class="s">"4.26"</span><span class="p">,</span>  
  <span class="n">pytorch_version</span><span class="o">=</span><span class="s">"1.13"</span><span class="p">,</span>
  <span class="n">py_version</span><span class="o">=</span><span class="s">'py39'</span><span class="p">,</span>           
<span class="p">)</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">huggingface_model</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
  <span class="n">initial_instance_count</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
  <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.m5.xlarge'</span>
<span class="p">)</span>
</code></pre></div></div>

<p>现在运行一下推理试试：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">res</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"在 SageMaker 上部署 Huggingface 模型"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">res</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div></div>

<p>结果如下：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>512 [-0.08045165985822678, 0.028644787147641182, 0.01914004050195217, -0.02148452214896679, 0.017731796950101852, -0.05328822508454323, 0.004828637931495905, 0.015703866258263588, -0.017463965341448784, -0.01227850466966629]
</code></pre></div></div>

<p>测试相似度：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">res1</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"抱歉"</span><span class="p">)</span>
<span class="n">res2</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"Sorry"</span><span class="p">)</span>
<span class="n">sim1</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">(</span><span class="n">res1</span><span class="p">,</span> <span class="n">res2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim1</span><span class="p">)</span>
</code></pre></div></div>

<p>结果为：<code class="language-plaintext highlighter-rouge">0.9197762458059101</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sim2</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">(</span><span class="n">res1</span><span class="p">,</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"对不起"</span><span class="p">))</span>
<span class="n">sim3</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">(</span><span class="n">res1</span><span class="p">,</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"Je mi líto"</span><span class="p">))</span>
<span class="n">sim4</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">(</span><span class="n">res1</span><span class="p">,</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"ごめんなさい"</span><span class="p">))</span>
<span class="n">sim5</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">(</span><span class="n">res1</span><span class="p">,</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"吃了吗"</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim2</span><span class="p">,</span> <span class="n">sim3</span><span class="p">,</span> <span class="n">sim4</span><span class="p">,</span> <span class="n">sim5</span><span class="p">)</span>
</code></pre></div></div>

<p>结果为：<code class="language-plaintext highlighter-rouge">0.9926941692158543 0.9510660558985051 0.9787457828105314 0.27865827356290396</code></p>

<p>比较符合预期！</p>

<h2 id="总结">总结</h2>

<p>上面的过程是在本地的 VSCode Notebook 中完成。</p>

<p>模型部署模型时间为 3 分 15 秒，我部署了 2 个实例，在实际的推理中，可以感觉到推理效率显著提高。</p>

<hr />
<p>相关</p>

<p><a href="deploy-huggingface-model-2-sagemaker-1.html">在 SageMaker 上部署 Huggingface 模型 (一)</a></p>

<p>参考</p>

<p><a href="https://github.com/aws/sagemaker-huggingface-inference-toolkit">https://github.com/aws/sagemaker-huggingface-inference-toolkit</a></p>

<p><a href="https://huggingface.co/docs/sagemaker/reference#inference-dlc-overview">Huggingface 默认支持的库版本对照</a></p>]]></content><author><name>啤酒云</name></author><category term="aiml," /><category term="aws" /><summary type="html"><![CDATA[在 Huggingface 上，有些模型并没有 Deploy - Sagemaker 这个功能，或者我们需要做一些特殊的任务，怎么办？本文介绍了如何让 SageMaker 调用自定义的推理代码。]]></summary></entry><entry><title type="html">在 SageMaker 上部署 Huggingface 模型 (一)</title><link href="https://youbug.cn/2023/05/deploy-huggingface-model-2-sagemaker-1.html" rel="alternate" type="text/html" title="在 SageMaker 上部署 Huggingface 模型 (一)" /><published>2023-05-10T02:13:33+00:00</published><updated>2023-05-10T02:13:33+00:00</updated><id>https://youbug.cn/2023/05/deploy-huggingface-model-2-sagemaker-1</id><content type="html" xml:base="https://youbug.cn/2023/05/deploy-huggingface-model-2-sagemaker-1.html"><![CDATA[<p>在 Huggingface 上浏览模型的时候，会看到一个 Deploy 按钮，很多模型点开会看到 Amazon SageMaker 选项，然后会看到一段代码。今天便试了一下这个，下面是测试过程：在本机运行运行代码，把 Huggingface 的 模型部署到 SageMaker 上并运行推理。</p>

<h2 id="创建-iam-角色">创建 IAM 角色</h2>

<p>往 SageMaker 部署模型需要一个角色，先创建一个角色，如果你之前运行过 SageMaker，大抵应该有了这个角色。</p>

<ul>
  <li>Trusted entity type: AWS service</li>
  <li>Use cases for other AWS services: SageMaker</li>
  <li>勾选上 SageMaker - Execution</li>
</ul>

<p>你会看到他已经选择了 AmazonSageMakerFullAccess 策略，点击下一步。</p>

<p>把这个角色命名为 HuggingfaceExecuteRole  或者其他。</p>

<h2 id="代码分段解读">代码分段解读</h2>

<blockquote>
  <p>你可以在 VSCode 里创建一个 xxx.ipynb 的文件，分段执行下面的代码。VSCode 提供了 python 和 notebook 相关插件。</p>

  <p>另外，你需要在本机配置正确的 AKSK 和 AWS Region。</p>

  <p>boto3 和 sagemaker 这俩依赖库要在本机安装一下。</p>
</blockquote>

<h3 id="定义模型">定义模型</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>  
<span class="kn">from</span> <span class="nn">sagemaker.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceModel</span>

<span class="n">hub</span> <span class="o">=</span>  <span class="p">{</span>
 <span class="s">'HF_MODEL_ID'</span><span class="p">:</span><span class="s">'bert-base-chinese'</span><span class="p">,</span>
 <span class="s">'HF_TASK'</span><span class="p">:</span><span class="s">'fill-mask'</span>
<span class="p">}</span>
</code></pre></div></div>

<ul>
  <li>这里定义了模型的名字和需要做的任务名称，SageMaker 会动态拉取 Huggingface 的模型。</li>
</ul>

<h3 id="部署模型">部署模型</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">iam_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="s">'iam'</span><span class="p">)</span>
<span class="n">role</span> <span class="o">=</span> <span class="n">iam_client</span><span class="p">.</span><span class="n">get_role</span><span class="p">(</span><span class="n">RoleName</span><span class="o">=</span><span class="s">'HuggingfaceExecuteRole'</span><span class="p">)</span>

<span class="n">huggingface_model</span> <span class="o">=</span> <span class="n">HuggingFaceModel</span><span class="p">(</span>
 <span class="n">transformers_version</span><span class="o">=</span><span class="s">'4.17.0'</span><span class="p">,</span>
 <span class="n">pytorch_version</span><span class="o">=</span><span class="s">'1.10.2'</span><span class="p">,</span>
 <span class="n">py_version</span><span class="o">=</span><span class="s">'py38'</span><span class="p">,</span>
 <span class="n">env</span><span class="o">=</span><span class="n">hub</span><span class="p">,</span>
 <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">[</span><span class="s">'Role'</span><span class="p">][</span><span class="s">'Arn'</span><span class="p">],</span> 
<span class="p">)</span>


<span class="n">predictor</span> <span class="o">=</span> <span class="n">huggingface_model</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
 <span class="n">initial_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
 <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.m5.xlarge'</span> 
<span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>这一步我们定义了模型运行的环境，角色和运行的机型，数量。</li>
  <li>role 这里需要注意一下，他的入参是 Role 的 Arn。</li>
  <li>这一步完成之后，可以到 SageMaker 后台看到计算资源/Endpoint 已经成功部署，位置在 Inference - Endpoints。</li>
</ul>

<h3 id="运行推理">运行推理</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">({</span>
 <span class="s">'inputs'</span><span class="p">:</span> <span class="s">"哈哈，我正在吃[MASK]。"</span>
<span class="p">})</span>
</code></pre></div></div>

<p>结果为：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[{</span><span class="s1">'score'</span>: 0.26574137806892395,
  <span class="s1">'token'</span>: 1450,
  <span class="s1">'token_str'</span>: <span class="s1">'呢'</span>,
  <span class="s1">'sequence'</span>: <span class="s1">'哈 哈 ， 我 正 在 吃 呢 。'</span><span class="o">}</span>,
 <span class="o">{</span><span class="s1">'score'</span>: 0.23924605548381805,
  <span class="s1">'token'</span>: 7649,
  <span class="s1">'token_str'</span>: <span class="s1">'饭'</span>,
  <span class="s1">'sequence'</span>: <span class="s1">'哈 哈 ， 我 正 在 吃 饭 。'</span><span class="o">}</span>,
 <span class="o">{</span><span class="s1">'score'</span>: 0.07031755149364471,
  <span class="s1">'token'</span>: 1557,
  <span class="s1">'token_str'</span>: <span class="s1">'啊'</span>,
  <span class="s1">'sequence'</span>: <span class="s1">'哈 哈 ， 我 正 在 吃 啊 。'</span><span class="o">}</span>,
 <span class="o">{</span><span class="s1">'score'</span>: 0.02992616966366768,
  <span class="s1">'token'</span>: 1521,
  <span class="s1">'token_str'</span>: <span class="s1">'哦'</span>,
  <span class="s1">'sequence'</span>: <span class="s1">'哈 哈 ， 我 正 在 吃 哦 。'</span><span class="o">}</span>,
 <span class="o">{</span><span class="s1">'score'</span>: 0.029010694473981857,
  <span class="s1">'token'</span>: 7613,
  <span class="s1">'token_str'</span>: <span class="s1">'飯'</span>,
  <span class="s1">'sequence'</span>: <span class="s1">'哈 哈 ， 我 正 在 吃 飯 。'</span><span class="o">}]</span>
</code></pre></div></div>

<h3 id="清理资源">清理资源</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictor</span><span class="p">.</span><span class="n">delete_endpoint</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="总结">总结</h2>

<p>整个过程还是非常的方便，Endpoint 的创建的速度非常快（本示例用了 2 分半钟），省去了安装运行环境的痛苦，爽的一匹啊！</p>

<hr />
<p>相关</p>

<p><a href="deploy-huggingface-model-2-sagemaker-2.html">在 SageMaker 上部署 Huggingface 模型 (二)</a></p>]]></content><author><name>啤酒云</name></author><category term="aiml," /><category term="aws" /><summary type="html"><![CDATA[在 Huggingface 上浏览模型的时候，会看到一个 Deploy 按钮，很多模型点开会看到 Amazon SageMaker 选项，然后会看到一段代码。今天便试了一下这个，下面是测试过程：在本机运行运行代码，把 Huggingface 的 模型部署到 SageMaker 上并运行推理。]]></summary></entry></feed>